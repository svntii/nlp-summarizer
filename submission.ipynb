{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/saint/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections.abc\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import tqdm as progressbar\n",
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETAILS_JSON = \"data/email_thread_details.json\"\n",
    "SUMMARIES_JSON = \"data/email_thread_summaries.json\"\n",
    "\n",
    "kThreadId = \"thread_id\"\n",
    "kSubject = \"subject\"\n",
    "kTimestamp = \"timestamp\"\n",
    "kFrom = \"from\"\n",
    "kTo = \"to\"\n",
    "kBody = \"body\"\n",
    "\n",
    "kSummary = \"summary\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "- Reduced vocab from 172743 to 16856\n",
    "- `#` WOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils():\n",
    "    @staticmethod\n",
    "    def load_dataset(DETAILS_FILE, SUMMARIES_FILE):\n",
    "        '''\n",
    "            This function loads the dataset from the file\n",
    "            ARGS:\n",
    "                filename: the name of the file\n",
    "            RETURN:\n",
    "                dataset: the dataset\n",
    "        '''\n",
    "        with open(DETAILS_FILE, 'r') as f:\n",
    "            details = json.load(f)\n",
    "        \n",
    "        with open(SUMMARIES_FILE, 'r') as f:\n",
    "            summaries = json.load(f)   \n",
    "        \n",
    "        dataset = {}\n",
    "        for i in range(len(details)):\n",
    "            item = details[i]\n",
    "            thread_id = item[kThreadId]\n",
    "            item = Utils.tokenize_body(item)\n",
    "            dataset[thread_id] = dataset.get(thread_id, []) + [item]\n",
    "        \n",
    "        for i in range(len(summaries)):\n",
    "            item = summaries[i]\n",
    "            thread_id = item[kThreadId]\n",
    "            item = Utils.tokenize_summary(item)\n",
    "            dataset[thread_id] = (dataset.get(thread_id), item)\n",
    "\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_body(item):\n",
    "        item[kBody] = word_tokenize(item[kBody])\n",
    "        item[kBody] = [re.sub(r'[^\\w\\s.]', '', word) for word in item[kBody]]\n",
    "        item[kBody] = [word.strip() for word in item[kBody] if word.strip() and word.strip() not in ['--', '=']]\n",
    "        # Lowercase the email body\n",
    "        item[kBody] = [word.lower() for word in item[kBody]]\n",
    "        item[kBody] = [\"<BOS>\"] + item[kBody] + [\"<EOS>\"]\n",
    "        item[kBody] = \" \".join(item[kBody])\n",
    "        return item\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_summary(item):\n",
    "        item[kSummary] = word_tokenize(item[kSummary])\n",
    "        item[kSummary] = [re.sub(r'[^\\w\\s.]', '', word) for word in item[kSummary]]\n",
    "        item[kSummary] = [word.strip() for word in item[kSummary] if word.strip() and word.strip() not in ['--', '=']]\n",
    "        # Lowercase the summary\n",
    "        item[kSummary] = [word.lower() for word in item[kSummary]]\n",
    "        item[kSummary] = \"<BOS> \" + \" \".join(item[kSummary]) + \"<EOS>\"\n",
    "        return item\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_vocab(data):\n",
    "        '''\n",
    "            This function builds the vocabulary from the data\n",
    "            ARGS:\n",
    "                data: the data to build the vocabulary from ([Email], EmailSummaries)\n",
    "            RETURN:\n",
    "                vocab: the vocabulary\n",
    "        '''\n",
    "        vocab = Vocab()\n",
    "        for _, (email_list, summary) in data.items():\n",
    "            for email in email_list:\n",
    "\n",
    "                for word in email:\n",
    "                    vocab.add(word)\n",
    "            for word in summary[kSummary].split():\n",
    "                vocab.add(word)\n",
    "        \n",
    "        return vocab\n",
    "\n",
    "class Vocab(collections.abc.MutableSet):\n",
    "    \"\"\"\n",
    "        Set-like data structure that can change words into numbers and back.\n",
    "        From Prof. David Chiang Code\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        words = {'<BOS>', '<EOS>', '<UNK>'}\n",
    "        self.num_to_word = list(words)\n",
    "        self.word_to_num = {word:num for num, word in enumerate(self.num_to_word)}\n",
    "    def add(self, word):\n",
    "        if word in self: return\n",
    "        num = len(self.num_to_word)\n",
    "        self.num_to_word.append(word)\n",
    "        self.word_to_num[word] = num\n",
    "    def discard(self, word):\n",
    "        raise NotImplementedError()\n",
    "    def update(self, words):\n",
    "        self |= words\n",
    "    def __contains__(self, word):\n",
    "        return word in self.word_to_num\n",
    "    def __len__(self):\n",
    "        return len(self.num_to_word)\n",
    "    def __iter__(self):\n",
    "        return iter(self.num_to_word)\n",
    "\n",
    "    def numberize(self, word):\n",
    "        \"\"\"Convert a word into a number.\"\"\"\n",
    "        if word in self.word_to_num:\n",
    "            return self.word_to_num[word]\n",
    "        else:\n",
    "            return self.word_to_num['<UNK>']\n",
    "\n",
    "    def denumberize(self, num):\n",
    "        \"\"\"Convert a number into a word.\"\"\"\n",
    "        return self.num_to_word[num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "d = Utils.load_dataset(DETAILS_JSON, SUMMARIES_JSON)\n",
    "vocab = Utils.build_vocab(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dictionary into train and test\n",
    "data = list(d.items())\n",
    "random.shuffle(data)\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train = [(email_list, summary) for _, (email_list, summary) in train]\n",
    "test = [(email_list, summary) for _, (email_list, summary) in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16831 16856\n"
     ]
    }
   ],
   "source": [
    "# print(ntokens, len(vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer(nn.Transformer):\n",
    "    '''\n",
    "    This class implements the summarizer\n",
    "    '''\n",
    "\n",
    "    def __init__(self, vocab_size, vocab, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        '''\n",
    "        This function initializes the model\n",
    "        ARGS:\n",
    "            vocab_size: the size of the vocabulary\n",
    "            d_model: the dimension of the model\n",
    "            nhead: the number of heads\n",
    "            num_encoder_layers: the number of encoder layers\n",
    "            num_decoder_layers: the number of decoder layers\n",
    "        RETURN:\n",
    "            None\n",
    "        '''\n",
    "\n",
    "        super(Summarizer, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # Embedding layer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_encoder_layers, \n",
    "            num_decoder_layers=num_decoder_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.kGreedy = \"greedy\"\n",
    "        self.kTopP = \"top_p\"\n",
    "        self.kBeam = \"beam\"\n",
    "\n",
    "    def forward(self, src, target=None):\n",
    "        '''\n",
    "        This function performs the forward pass of the model\n",
    "        ARGS:\n",
    "            src: the source input\n",
    "            target: the target input (optional, used during training)\n",
    "        RETURN:\n",
    "            output: the output of the model\n",
    "        '''\n",
    "        # reduce the size of the src\n",
    "\n",
    "\n",
    "        src = self.embedding(src)\n",
    "        src = torch.mean(src, dim=0)\n",
    "        \n",
    "        if target is not None:\n",
    "            target = self.embedding(target)\n",
    "            src = src.unsqueeze(0).expand(target.size(0), -1)\n",
    "            output = self.transformer(src, target)\n",
    "        else:\n",
    "            # In generation mode, don't use target\n",
    "            src = src.unsqueeze(0).expand(src.size(0), -1)\n",
    "            output = self.transformer(src, src)  # Use src as both source and target TODO: \n",
    "        \n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def summarize(self, src, max_len=100, mode=\"top_p\"):\n",
    "        '''\n",
    "        This function summarize the input text\n",
    "            args:\n",
    "                src: the source input\n",
    "                max_len: the maximum length of the output\n",
    "                mode: the mode of generation (greedy or beam search)\n",
    "            return:\n",
    "                output: the output of the model\n",
    "        '''\n",
    "        src = torch.tensor([self.vocab.numberize(word) for word in src])\n",
    "        o = self.forward(src)\n",
    "        output = None\n",
    "        \n",
    "        if mode == self.kGreedy:\n",
    "            output =  self.greedy_decoding(o, max_len)\n",
    "        elif mode == self.kTopP:\n",
    "            output = self.top_p_decoding(o, max_len)\n",
    "        elif mode == self.kBeam:\n",
    "            output = self.beam_search(o, max_len)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def greedy_decoding(self, o, max_len):\n",
    "        '''\n",
    "        This function performs greedy decoding\n",
    "        ARGS:\n",
    "            o: the output of the model\n",
    "            max_len: the maximum length of the output\n",
    "        RETURN:\n",
    "            output: the output of the model\n",
    "        '''\n",
    "        output = []\n",
    "        words = 0\n",
    "        for i in o:\n",
    "            if words >= max_len:\n",
    "                break\n",
    "            a = torch.argmax(i)\n",
    "            if a == self.vocab.numberize(\"<EOS>\"):\n",
    "                break\n",
    "            a = self.vocab.denumberize(a)\n",
    "            output.append(a)\n",
    "            words += 1\n",
    "        return output\n",
    "    \n",
    "    def top_p_decoding(self, o, max_len = 50, p=0.9):\n",
    "        '''\n",
    "        This function performs top-p decoding\n",
    "        ARGS:\n",
    "            o: the output of the model\n",
    "            max_len: the maximum length of the output\n",
    "            p: the probability threshold\n",
    "        RETURN:\n",
    "            output: the output of the model\n",
    "        '''\n",
    "        output = []\n",
    "        words = 0\n",
    "        for i in o:\n",
    "            if words >= max_len:\n",
    "                break\n",
    "            sorted_prob, sorted_idx = torch.sort(i, descending=False)\n",
    "            sorted_prob = torch.exp(sorted_prob)\n",
    "            sorted_prob_cumsum = sorted_prob.cumsum(dim=0)\n",
    "            top_p_batch = sorted_idx[sorted_prob_cumsum > p]\n",
    "\n",
    "            if top_p_batch.nelement() > 0:\n",
    "                next_token = random.choice(top_p_batch)\n",
    "                output.append(next_token.item())\n",
    "            else:\n",
    "                next_token = random.choice(sorted_idx)\n",
    "                output.append(next_token.item())\n",
    "            \n",
    "            if output[-1] == self.vocab.numberize(\"<EOS>\"):\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        for i, tensor in enumerate(output):\n",
    "            output[i] = self.vocab.denumberize(tensor)\n",
    "    \n",
    "        return output\n",
    "\n",
    "    def beam_search(self, o, max_len):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab) # size of vocabulary\n",
    "emsize = 150 # embedding dimension\n",
    "nhid = 150 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "n_encoder_layers = 6 # the number of encoder layers\n",
    "n_decoder_layers = 6 # the number of decoder layers\n",
    "nhead = 3 # the number of heads in the multiheadattention models\n",
    "lr = 0.001 # learning rate\n",
    "\n",
    "model = Summarizer(ntokens, vocab, emsize, nhead, n_encoder_layers, n_decoder_layers)\n",
    "\n",
    "# RECENT_MODEL = \"models/model.pt-2023-12-04_15_40_57.pt\"\n",
    "# model.load_state_dict(torch.load(RECENT_MODEL, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_summarizer(model: Summarizer, train_data, criterion, max_input_len = 150, max_output_len = 50, lr=0.001, threshold_norm=0.5):\n",
    "    model.train()  # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Initialize Adam optimizer\n",
    "\n",
    "    for item in progressbar.tqdm(train_data, desc=\"Thread Training\", total=len(train_data)):\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        thread, summary = item\n",
    "\n",
    "        email_thread_body = [email[kBody] for email in thread]\n",
    "        email_thread_body = \" \".join(email_thread_body)\n",
    "\n",
    "        email_tensor = torch.tensor([model.vocab.numberize(word) for word in email_thread_body.split()]) # Convert email to tensor\n",
    "        summary_tensor = torch.tensor([model.vocab.numberize(word) for word in summary[kSummary].split()]) # Convert summary to tensor\n",
    "        output = model(email_tensor, summary_tensor)  # Forward pass\n",
    "        \n",
    "        output = output.view(-1, ntokens) # Reshape output \n",
    "        loss = criterion(output, summary_tensor) # Calculate loss\n",
    "        loss.backward() # Backward pass\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), threshold_norm) # Clip gradients\n",
    "        optimizer.step() # Update weights\n",
    "        total_loss += loss.item() \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread Training: 100%|██████████| 3333/3333 [05:05<00:00, 10.90it/s]\n",
      "Thread Training: 100%|██████████| 3333/3333 [10:27<00:00,  5.31it/s]  \n",
      "Thread Training: 100%|██████████| 3333/3333 [15:15<00:00,  3.64it/s]   \n",
      "Thread Training: 100%|██████████| 3333/3333 [08:21<00:00,  6.65it/s]\n",
      "Thread Training: 100%|██████████| 3333/3333 [08:29<00:00,  6.55it/s]\n",
      "Thread Training: 100%|██████████| 3333/3333 [08:05<00:00,  6.87it/s]  \n",
      "Thread Training: 100%|██████████| 3333/3333 [07:48<00:00,  7.11it/s]  \n",
      "Thread Training: 100%|██████████| 3333/3333 [05:46<00:00,  9.61it/s]\n",
      "Thread Training: 100%|██████████| 3333/3333 [05:00<00:00, 11.10it/s]\n",
      "Thread Training: 100%|██████████| 3333/3333 [09:54<00:00,  5.60it/s]  \n",
      "Thread Training: 100%|██████████| 3333/3333 [11:09<00:00,  4.97it/s]   \n",
      "Thread Training: 100%|██████████| 3333/3333 [08:18<00:00,  6.69it/s]   \n",
      "Thread Training: 100%|██████████| 3333/3333 [08:49<00:00,  6.29it/s]  \n",
      "Thread Training: 100%|██████████| 3333/3333 [16:29<00:00,  3.37it/s]   \n",
      "Thread Training: 100%|██████████| 3333/3333 [04:57<00:00, 11.19it/s]\n"
     ]
    }
   ],
   "source": [
    "curr_time = time.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "MODEL_PATH = f\"models/model.pt-{curr_time}.pt\"\n",
    "for i in range(15):\n",
    "    print(f\"Epoch {i}/ {15}\")\n",
    "    model = train_summarizer(model, train, nn.CrossEntropyLoss(), lr=lr)\n",
    "# save the model\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Summarizer:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([16831, 150]) from checkpoint, the shape in current model is torch.Size([16856, 150]).\n\tsize mismatch for fc_out.weight: copying a param with shape torch.Size([16831, 150]) from checkpoint, the shape in current model is torch.Size([16856, 150]).\n\tsize mismatch for fc_out.bias: copying a param with shape torch.Size([16831]) from checkpoint, the shape in current model is torch.Size([16856]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/saint/Desktop/school/NLP/nlp-summarizer/submission.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saint/Desktop/school/NLP/nlp-summarizer/submission.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# load the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saint/Desktop/school/NLP/nlp-summarizer/submission.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m RECENT_MODEL \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodels/model.pt-2023-12-05_18:16:19.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/saint/Desktop/school/NLP/nlp-summarizer/submission.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(RECENT_MODEL, map_location\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Summarizer:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([16831, 150]) from checkpoint, the shape in current model is torch.Size([16856, 150]).\n\tsize mismatch for fc_out.weight: copying a param with shape torch.Size([16831, 150]) from checkpoint, the shape in current model is torch.Size([16856, 150]).\n\tsize mismatch for fc_out.bias: copying a param with shape torch.Size([16831]) from checkpoint, the shape in current model is torch.Size([16856])."
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "RECENT_MODEL = \"models/model.pt-2023-12-05_18:16:19.pt\"\n",
    "model.load_state_dict(torch.load(RECENT_MODEL, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the output\n",
    "\n",
    "def evaluate(model: Summarizer, test_data, criterion, rouge, max_input_len = 150, max_output_len = 50, mode=\"greedy\"):\n",
    "    model.eval()  # Turn on the evaluation mode\n",
    "    \n",
    "    total_loss = 0.\n",
    "    evals = []\n",
    "    with torch.no_grad():\n",
    "        for item in progressbar.tqdm(test_data, desc=\"Thread Evaluation\", total=len(test_data)):\n",
    "            thread, summary = item\n",
    "            summary_string = summary[kSummary]\n",
    "\n",
    "\n",
    "\n",
    "            email_thread_body = [email[kBody] for email in thread]\n",
    "            email_thread_body = \" \".join(email_thread_body)\n",
    "\n",
    "            output = model.summarize(email_thread_body, mode=\"top_p\")\n",
    "        \n",
    "            # loss = criterion(output, summary_string)\n",
    "            # total_loss += loss.item()\n",
    "            output_str = \" \".join(output)\n",
    "            rouge_score = rouge.compute(predictions=[output_str], references=[summary_string])\n",
    "\n",
    "            evals.append((summary[kThreadId], output_str, summary_string ,rouge_score))\n",
    "    \n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread Evaluation: 100%|██████████| 4/4 [00:01<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "evals = evaluate(model, test[:4], nn.CrossEntropyLoss(), rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread ID: 1939\n",
      "Output: bombay citizens hydro indeed imbalances rtu crossborder lindo thane appeased roles fcms supervisor gir web peer ondreko dianne serc audiovideo but 1997 liquid reactions lakshmi myrna bnew screen firstly lbj donald hauser mickum 13th regime disclaimers midjanuary sefton lesli regressions eisel rebooking rosalee stopping ideas marketingorigination doubts bullet inform charlie pricebasisindex galvan tribolet 18xx worstcase denominated ameren kaye expectation farallonoaktree highest buyback replication discounts logistic storage plays dealings wanted advantage lokey kochzone richmond resort circularity reflect immediate finds 5th orleans indefinitely quilkey crunchers villarreal asap 4309724 pete edge responsibilities desleigh intramonth horn prince vents arctic 720959 relaxing huang obs highlighted administrators merging 1143983 johanson producers timothy funds bond giant wos woodrow restaurants 30496 nov lookups germany spam macbarron turcich radio paystubs gigi beauty 5.7 hazardous regards cornhusker air eligibility designshop interviewees revenge trust performing swg listening autoborrow complications 6999 richmond criticizing paralegals adjusted estimate lobbying numbers tattoo bpa among 56943\n",
      "Summary: vince sends john a resume from his friend mark kierlanczyk who is seeking career advice and opportunities in the energy markets . mark highlights his experience in finance including working at goldman sachs and salomon brothers as well as his ph.d. in mathematics from mit . vince also sends louise a resume from a candidate who graduated from the same university in poland as vince and has an additional degree in ecommerce . vince informs john that this candidate is currently under freeze as an ei employee .\n",
      "Score: {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "for i in evals:\n",
    "    thread_id = i[0]\n",
    "    output = i[1]\n",
    "    summary = i[2]\n",
    "    score = i[3]\n",
    "    print(f\"Thread ID: {thread_id}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print(f\"Summary: {summary}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['started',\n",
       " 'round',\n",
       " 'legislators',\n",
       " 'needing',\n",
       " 'notebooks',\n",
       " 'publisher',\n",
       " 'expertfinder',\n",
       " 'rain',\n",
       " 'prudential',\n",
       " 'antiterrorist',\n",
       " '916',\n",
       " 'topsoe',\n",
       " '42nd',\n",
       " 'agreement',\n",
       " 'sol',\n",
       " 'postlethwaite',\n",
       " 'regime',\n",
       " 'goes',\n",
       " 'amsterdam',\n",
       " 'gruene',\n",
       " 'manual',\n",
       " 'transition',\n",
       " '9c2',\n",
       " 'dietary',\n",
       " 'formula',\n",
       " 'thackray',\n",
       " 'explorer',\n",
       " 'check',\n",
       " 'vacancy',\n",
       " 'pursuits',\n",
       " 'doing',\n",
       " 'restful',\n",
       " 'analyst',\n",
       " 'prepopulated',\n",
       " 'dec',\n",
       " 'turcich',\n",
       " 'jimmy',\n",
       " 'gras',\n",
       " 'intervention',\n",
       " 'explore',\n",
       " 'canada',\n",
       " 'golfing',\n",
       " 'ets',\n",
       " 'declines',\n",
       " 'barrow',\n",
       " 'pulled',\n",
       " 'identities',\n",
       " 'rally',\n",
       " 'scientific',\n",
       " 'unconstrained',\n",
       " 'flash1.jpg',\n",
       " 'given',\n",
       " 'blevins',\n",
       " 'slip',\n",
       " 'perpetuity',\n",
       " 'seasons',\n",
       " 'habiba',\n",
       " 'eb5c2',\n",
       " 'interruptibility',\n",
       " 'lardy',\n",
       " 'nymex',\n",
       " '33100',\n",
       " 'squeezing',\n",
       " 'raiders',\n",
       " 'enrondelta',\n",
       " 'nopr',\n",
       " 'rmt',\n",
       " 'gockerman',\n",
       " 'joy',\n",
       " 'opvspt',\n",
       " '1303',\n",
       " 'frustrated',\n",
       " 'cents',\n",
       " 'twice',\n",
       " 'jr',\n",
       " 'overpulling',\n",
       " 'trail',\n",
       " 'belong',\n",
       " 'debts',\n",
       " 'watched',\n",
       " 'westwide',\n",
       " 'profiled',\n",
       " 'nevius',\n",
       " 'acceleration',\n",
       " 'bbq',\n",
       " 'hospitality',\n",
       " 'sempra',\n",
       " 'clip',\n",
       " 'breakeven',\n",
       " '10162000',\n",
       " 'sympathy',\n",
       " 'permian',\n",
       " 'tnpc',\n",
       " 'showcases',\n",
       " 'boyer',\n",
       " 'glen',\n",
       " 'curt',\n",
       " 'martinez',\n",
       " 'servers',\n",
       " 'mainzer',\n",
       " 'projector',\n",
       " 'interests',\n",
       " 'lp',\n",
       " 'boundaries',\n",
       " 'receivables',\n",
       " 'katyplttailgate',\n",
       " 'shui',\n",
       " 'raise',\n",
       " 'suit',\n",
       " 'josie',\n",
       " 'headbanging',\n",
       " 'openness',\n",
       " 'pounds',\n",
       " 'lobby',\n",
       " 'raises',\n",
       " '1126',\n",
       " 'guaranty',\n",
       " 'manner',\n",
       " 'stub',\n",
       " '100pm',\n",
       " 'allstar',\n",
       " 'expansion',\n",
       " 'both',\n",
       " 'timothy',\n",
       " 'pressures',\n",
       " 'feelings',\n",
       " 'affiliation',\n",
       " '0.55',\n",
       " 'corruption',\n",
       " '872000',\n",
       " 'proportions',\n",
       " 'sb',\n",
       " 'stephen',\n",
       " 'usd',\n",
       " 'xerox',\n",
       " 'hi',\n",
       " 'elyse',\n",
       " 'borrowing',\n",
       " 'injections',\n",
       " 'nervous',\n",
       " 'pointed',\n",
       " 'validation',\n",
       " 'migration',\n",
       " 'minter',\n",
       " 'gleason',\n",
       " 'prebon',\n",
       " 'interpretation',\n",
       " 'marcinkowski',\n",
       " 'reorganization',\n",
       " 'amusement']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summarize(\"hello world\", mode=\"top_p\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
