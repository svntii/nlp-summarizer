{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections.abc\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm as progressbar\n",
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETAILS_JSON = \"data/email_thread_details.json\"\n",
    "SUMMARIES_JSON = \"data/email_thread_summaries.json\"\n",
    "\n",
    "kThreadId = \"thread_id\"\n",
    "kSubject = \"subject\"\n",
    "kTimestamp = \"timestamp\"\n",
    "kFrom = \"from\"\n",
    "kTo = \"to\"\n",
    "kBody = \"body\"\n",
    "kSummary = \"summary\"\n",
    "\n",
    "BOS = \"<BOS>\"\n",
    "EOS = \"<EOS>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "- Reduced vocab from 172743 to 16856\n",
    "- `#` WOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils():\n",
    "    @staticmethod\n",
    "    def load_dataset(DETAILS_FILE, SUMMARIES_FILE):\n",
    "        '''\n",
    "            This function loads the dataset from the file\n",
    "            ARGS:\n",
    "                filename: the name of the file\n",
    "            RETURN:\n",
    "                dataset: the dataset\n",
    "        '''\n",
    "        with open(DETAILS_FILE, 'r') as f:\n",
    "            details = json.load(f)\n",
    "        \n",
    "        with open(SUMMARIES_FILE, 'r') as f:\n",
    "            summaries = json.load(f)   \n",
    "        \n",
    "        dataset = {}\n",
    "        for i in range(len(details)):\n",
    "            item = details[i]\n",
    "            thread_id = item[kThreadId]\n",
    "            item = Utils.tokenize_body(item)\n",
    "            dataset[thread_id] = dataset.get(thread_id, []) + [item]\n",
    "        \n",
    "        for i in range(len(summaries)):\n",
    "            item = summaries[i]\n",
    "            thread_id = item[kThreadId]\n",
    "            item = Utils.tokenize_summary(item)\n",
    "            dataset[thread_id] = (dataset.get(thread_id), item)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_body(item):\n",
    "        sentences = sent_tokenize(item[kBody])\n",
    "        item[kBody] = [word_tokenize(sentence) for sentence in sentences]\n",
    "        item[kBody] = \" \".join([word for sentence in item[kBody] for word in sentence])\n",
    "        item[kBody] = \"<BOS> \" + item[kBody] + \" <EOS>\"\n",
    "        return item\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_summary(item):\n",
    "        sentences = sent_tokenize(item[kSummary])\n",
    "        item[kSummary] = [word_tokenize(sentence) for sentence in sentences]\n",
    "        item[kSummary] = \" \".join([word for sentence in item[kSummary] for word in sentence])\n",
    "        item[kSummary] = \"<BOS> \" + item[kSummary] + \" <EOS>\"\n",
    "        return item\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_vocab(data):\n",
    "        '''\n",
    "            This function builds the vocabulary from the data\n",
    "            ARGS:\n",
    "                data: the data to build the vocabulary from ([Email], EmailSummaries)\n",
    "            RETURN:\n",
    "                vocab: the vocabulary\n",
    "        '''\n",
    "        vocab = Vocab()\n",
    "        for _, (email_list, summary) in data.items():\n",
    "            for email in email_list:\n",
    "\n",
    "                for word in email:\n",
    "                    vocab.add(word)\n",
    "            for word in summary[kSummary].split():\n",
    "                vocab.add(word)\n",
    "        \n",
    "        return vocab\n",
    "\n",
    "class Vocab(collections.abc.MutableSet):\n",
    "    \"\"\"\n",
    "        Set-like data structure that can change words into numbers and back.\n",
    "        From Prof. David Chiang Code\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        words = {'<BOS>', '<EOS>', '<UNK>'}\n",
    "        self.num_to_word = list(words)\n",
    "        self.word_to_num = {word:num for num, word in enumerate(self.num_to_word)}\n",
    "    def add(self, word):\n",
    "        if word in self: return\n",
    "        num = len(self.num_to_word)\n",
    "        self.num_to_word.append(word)\n",
    "        self.word_to_num[word] = num\n",
    "    def discard(self, word):\n",
    "        raise NotImplementedError()\n",
    "    def update(self, words):\n",
    "        self |= words\n",
    "    def __contains__(self, word):\n",
    "        return word in self.word_to_num\n",
    "    def __len__(self):\n",
    "        return len(self.num_to_word)\n",
    "    def __iter__(self):\n",
    "        return iter(self.num_to_word)\n",
    "\n",
    "    def numberize(self, word):\n",
    "        \"\"\"Convert a word into a number.\"\"\"\n",
    "        if word in self.word_to_num:\n",
    "            return self.word_to_num[word]\n",
    "        else:\n",
    "            return self.word_to_num['<UNK>']\n",
    "\n",
    "    def denumberize(self, num):\n",
    "        \"\"\"Convert a number into a word.\"\"\"\n",
    "        return self.num_to_word[num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "d = Utils.load_dataset(DETAILS_JSON, SUMMARIES_JSON)\n",
    "vocab = Utils.build_vocab(d)\n",
    "len_vocab = len(vocab)\n",
    "print(\"Vocab Size: \", len_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dictionary into train and test\n",
    "data = list(d.items())\n",
    "random.shuffle(data)\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train = [(email_list, summary) for _, (email_list, summary) in train]\n",
    "test = [(email_list, summary) for _, (email_list, summary) in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Sample Email\n",
    "i = random.randint(0, len(train))\n",
    "print(f\"Sample Thread {i}: \")\n",
    "s = \" \".join([email[kBody] for email in train[i][0]])\n",
    "print(s)\n",
    "print(len(s.split()))\n",
    "print(\"Sample Summary: \")\n",
    "print(train[i][1][kSummary])\n",
    "print(len(train[i][1][kSummary].split()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer(nn.Transformer):\n",
    "    '''\n",
    "    This class implements the summarizer\n",
    "    '''\n",
    "\n",
    "    def __init__(self, vocab_size, vocab, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        '''\n",
    "        This function initializes the model\n",
    "        ARGS:\n",
    "            vocab_size: the size of the vocabulary\n",
    "            d_model: the dimension of the model\n",
    "            nhead: the number of heads\n",
    "            num_encoder_layers: the number of encoder layers\n",
    "            num_decoder_layers: the number of decoder layers\n",
    "        RETURN:\n",
    "            None\n",
    "        '''\n",
    "\n",
    "        super(Summarizer, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        self.pos = PositionalEncoding(\n",
    "            d_model, \n",
    "            0.1, \n",
    "            5000\n",
    "        )\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # Embedding layer\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_encoder_layers, \n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.kGreedy = \"greedy\"\n",
    "        self.kTopP = \"top_p\"\n",
    "        self.kBeam = \"beam\"\n",
    "\n",
    "    def forward(self, src:torch.TensorType, tgt:torch.TensorType, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        # Src size must be (batch_size, src sequence length)\n",
    "        # Tgt size must be (batch_size, tgt sequence length)\n",
    "\n",
    "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        src = self.pos(src)\n",
    "        tgt = self.pos(tgt)\n",
    "        \n",
    "        # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we permute\n",
    "        # to obtain size (sequence length, batch_size, dim_model),\n",
    "        src = src.permute(1,0,2)\n",
    "        tgt = tgt.permute(1,0,2)\n",
    "\n",
    "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
    "        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "        out = self.out(transformer_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        return (matrix == pad_token)\n",
    "    \n",
    "\n",
    "\n",
    "    def summarize(model, input_sequence, max_length=15, SOS_token=2, EOS_token=3, device='cuda'):\n",
    "        \"\"\"\n",
    "        Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "        Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n",
    "\n",
    "        num_tokens = len(input_sequence[0])\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Get source mask\n",
    "            tgt_mask = model.get_tgt_mask(y_input.size(1)).to(device)\n",
    "            \n",
    "            pred = model(input_sequence, y_input, tgt_mask)\n",
    "            \n",
    "            next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
    "            next_item = torch.tensor([[next_item]], device=device)\n",
    "\n",
    "            # Concatenate previous input with predicted best word\n",
    "            y_input = torch.cat((y_input, next_item), dim=1)\n",
    "\n",
    "            # Stop if model predicts end of sentence\n",
    "            if next_item.view(-1).item() == EOS_token:\n",
    "                break\n",
    "\n",
    "        return y_input.view(-1).tolist()\n",
    "\n",
    "    # def summarize(self, src, max_len=100, mode=\"top_p\"):\n",
    "    #     '''\n",
    "    #     This function summarize the input text\n",
    "    #         args:\n",
    "    #             src: the source input\n",
    "    #             max_len: the maximum length of the output\n",
    "    #             mode: the mode of generation (greedy or beam search)\n",
    "    #         return:\n",
    "    #             output: the output of the model\n",
    "    #     '''\n",
    "    #     src = torch.tensor([self.vocab.numberize(word) for word in src])\n",
    "    #     o = self.forward(src)\n",
    "    #     output = None\n",
    "        \n",
    "    #     if mode == self.kGreedy:\n",
    "    #         output =  self.greedy_decoding(o, max_len)\n",
    "    #     elif mode == self.kTopP:\n",
    "    #         output = self.top_p_decoding(o, max_len)\n",
    "    #     elif mode == self.kBeam:\n",
    "    #         output = self.beam_search(o, max_len)\n",
    "\n",
    "    #     return output\n",
    "        \n",
    "    def greedy_decoding(self, o, max_len):\n",
    "        '''\n",
    "        This function performs greedy decoding\n",
    "        ARGS:\n",
    "            o: the output of the model\n",
    "            max_len: the maximum length of the output\n",
    "        RETURN:\n",
    "            output: the output of the model\n",
    "        '''\n",
    "        output = []\n",
    "        words = 0\n",
    "        for i in o:\n",
    "            if words >= max_len:\n",
    "                break\n",
    "            a = torch.argmax(i)\n",
    "            if a == self.vocab.numberize(\"<EOS>\"):\n",
    "                break\n",
    "            a = self.vocab.denumberize(a)\n",
    "            output.append(a)\n",
    "            words += 1\n",
    "        return output\n",
    "    \n",
    "    def top_p_decoding(self, o, max_len = 50, p=0.9):\n",
    "        '''\n",
    "        This function performs top-p decoding\n",
    "        ARGS:\n",
    "            o: the output of the model\n",
    "            max_len: the maximum length of the output\n",
    "            p: the probability threshold\n",
    "        RETURN:\n",
    "            output: the output of the model\n",
    "        '''\n",
    "        output = []\n",
    "        words = 0\n",
    "        for i in o:\n",
    "            if words >= max_len:\n",
    "                break\n",
    "            sorted_prob, sorted_idx = torch.sort(i, descending=False)\n",
    "            sorted_prob = torch.exp(sorted_prob)\n",
    "            sorted_prob_cumsum = sorted_prob.cumsum(dim=0)\n",
    "            top_p_batch = sorted_idx[sorted_prob_cumsum > p]\n",
    "\n",
    "            if top_p_batch.nelement() > 0:\n",
    "                next_token = random.choice(top_p_batch)\n",
    "                output.append(next_token.item())\n",
    "            else:\n",
    "                next_token = random.choice(sorted_idx)\n",
    "                output.append(next_token.item())\n",
    "            \n",
    "            if output[-1] == self.vocab.numberize(\"<EOS>\"):\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        for i, tensor in enumerate(output):\n",
    "            output[i] = self.vocab.denumberize(tensor)\n",
    "    \n",
    "        return output\n",
    "\n",
    "    def beam_search(self, o, max_len):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab) # size of vocabulary\n",
    "emsize = 160 # embedding dimension\n",
    "nhid = 160 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "n_encoder_layers = 6 # the number of encoder layers\n",
    "n_decoder_layers = 6 # the number of decoder layers\n",
    "nhead = 8 # the number of heads in the multiheadattention models\n",
    "lr = 0.001 # learning rate\n",
    "\n",
    "model = Summarizer(ntokens, vocab, emsize, nhead, n_encoder_layers, n_decoder_layers).to(DEVICE)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# RECENT_MODEL = \"models/model.pt-2023-12-04_15_40_57.pt\"\n",
    "# model.load_state_dict(torch.load(RECENT_MODEL, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model:Summarizer, opt, loss_fn, training_data):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in progressbar(training_data, desc=\"Training\", file=sys.stdout, total=len(training_data)):\n",
    "        thread, summary = batch\n",
    "        thread_body = \" \".join([email[kBody] for email in thread])\n",
    "        thread_body = torch.Tensor([vocab.numberize(word) for word in thread_body.split()], dtype=torch.long, device=DEVICE)\n",
    "        summary = torch.Tensor([vocab.numberize(word) for word in summary[kSummary].split()], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "        # Now we shift the summary by one so with the <BOS> we predict the token at pos 1\n",
    "        summary_input = summary[:-1]\n",
    "        summary_expected = summary[1:]\n",
    "  \n",
    "        # Get mask to mask out the next words\n",
    "        sequence_length = summary_input.size(0)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(DEVICE)\n",
    "\n",
    "        predict = model(thread_body, summary_input, tgt_mask)\n",
    "\n",
    "        # Permute pred to have batch size first again\n",
    "        predict = predict.permute(1, 2, 0)\n",
    "        loss = loss_fn(predict, summary_expected)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "\n",
    "        break\n",
    "        \n",
    "    return total_loss / len(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = train_loop(model, opt, loss_fn, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, loss_fn, dev_data):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in progressbar(dev_data, desc=\"Validation\", file=sys.stdout, total=len(dev_data)):\n",
    "            thread, summary = batch\n",
    "            thread_body = \" \".join([email[kBody] for email in thread])\n",
    "            thread_body = torch.Tensor([vocab.numberize(word) for word in thread_body.split()], dtype=torch.long, device=DEVICE)\n",
    "            summary = torch.Tensor([vocab.numberize(word) for word in summary[kSummary].split()], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "            # Now we shift the tgt by one so with the <BOS> we predict the token at pos 1\n",
    "            summary_input = summary[:-1]\n",
    "            summary_expected = summary[1:]\n",
    "            \n",
    "            # Get mask to mask out the next words\n",
    "            sequence_length = summary_input.size(0)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(DEVICE)\n",
    "\n",
    "            # Standard training except we pass in y_input and src_mask\n",
    "            predict = model(thread_body, summary_input, tgt_mask)\n",
    "\n",
    "            # Permute pred to have batch size first again\n",
    "            predict = predict.permute(1, 2, 0)\n",
    "            loss = loss_fn(predict, summary_expected)\n",
    "            total_loss += loss.detach().item()\n",
    " \n",
    "        \n",
    "    return total_loss / len(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss = validation_loop(model, loss_fn, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Used for plotting later on\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "    \n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
    "        \n",
    "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list += [train_loss]\n",
    "        \n",
    "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        validation_loss_list += [validation_loss]\n",
    "        \n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print()\n",
    "        \n",
    "    return train_loss_list, validation_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train[:10]\n",
    "dev_subset = test[:10]\n",
    "\n",
    "train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_subset, dev_subset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_summarizer(model: Summarizer, train_data, dev_data, criterion, epochs=1, lr=0.003):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Initialize Adam optimizer\n",
    "    prev_dev_loss = best_dev_loss = None\n",
    "    model.train()  # Turn on the train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1} / {epochs}\")\n",
    "        random.shuffle(train_data)\n",
    "        train_loss = 0\n",
    "\n",
    "        for item in progressbar.tqdm(train_data, desc=\"Thread Training\", total=len(train_data)):\n",
    "            thread, summary = item\n",
    "            email_thread_body = [email[kBody] for email in thread]\n",
    "            email_thread_body = \" \".join(email_thread_body)\n",
    "\n",
    "            email_tensor = torch.tensor([model.vocab.numberize(word) for word in email_thread_body.split()], dtype=torch.int64).to(DEVICE) # Convert email to tensor\n",
    "            summary_tensor = torch.tensor([model.vocab.numberize(word) for word in summary[kSummary].split()],dtype=torch.int64).to(DEVICE) # Convert summary to tensor\n",
    "            \n",
    "            output = model(email_tensor, summary_tensor)  # Forward pass\n",
    "            summary_tensor = summary_tensor[1:] # Remove <BOS> token\n",
    "            loss = criterion(output, summary_tensor) # Calculate loss\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            loss.backward() # Backward pass\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), threshold_norm) # Clip gradients\n",
    "            optimizer.step() # Update weights\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        dev_loss = 0\n",
    "        evals = []\n",
    "        line_num = 0\n",
    "        for item in progressbar.tqdm(dev_data, desc=\"Thread Dev\", total=len(dev_data)):\n",
    "            thread, summary = item\n",
    "            summary_string = summary[kSummary]\n",
    "\n",
    "            email_thread_body = [email[kBody] for email in thread]\n",
    "            email_thread_body = \" \".join(email_thread_body)\n",
    "\n",
    "            output = model.summarize(email_thread_body, mode=\"top_p\", max_len=40)\n",
    "            output = \" \".join(output)\n",
    "            score = rouge.compute(predictions=[output], references=[summary_string])\n",
    "            evals.append((summary[kThreadId], output, summary_string, score))\n",
    "            if line_num < 5 and epoch % 5 == 0:\n",
    "                print(f\"Thread ID: {summary[kThreadId]}\")\n",
    "                print(f\"Email Thread: {email_thread_body}\")\n",
    "                print(f\"Predicted Summary: {output}\")\n",
    "                print(f\"Actual Summary: {summary_string}\")\n",
    "                print(f\"Score: {score}\")\n",
    "                print(\"-----------------------------------\")\n",
    "            line_num += 1\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}: Average Train Loss: {train_loss/len(train_data)}\",file=sys.stderr ,flush=True)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train[:10]\n",
    "dev_subset = test[:10]\n",
    "\n",
    "curr_time = time.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "MODEL_PATH = f\"models/model.pt-{curr_time}.pt\"\n",
    "\n",
    "model = train_summarizer(\n",
    "    model=model, \n",
    "    train_data=train, \n",
    "    dev_data=train_subset, \n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    epochs=10,\n",
    ")\n",
    "# save the model\n",
    "\n",
    "# torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join([email[kBody] for email in train_subset[0][0]])\n",
    "summary = train_subset[0][1][kSummary]\n",
    "o = model.summarize(text, max_len=50, mode=\"top_p\")\n",
    "o = \" \".join(o)\n",
    "print(o)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "# RECENT_MODEL = \"models/model.pt-2023-12-05_18:16:19.pt\"\n",
    "# model.load_state_dict(torch.load(RECENT_MODEL, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the output\n",
    "\n",
    "def evaluate(model: Summarizer, test_data, criterion, rouge, max_input_len = 150, max_output_len = 50, mode=\"greedy\"):\n",
    "    model.eval()  # Turn on the evaluation mode\n",
    "    \n",
    "    total_loss = 0.\n",
    "    evals = []\n",
    "    with torch.no_grad():\n",
    "        for item in progressbar.tqdm(test_data, desc=\"Thread Evaluation\", total=len(test_data)):\n",
    "            thread, summary = item\n",
    "            summary_string = summary[kSummary]\n",
    "\n",
    "            email_thread_body = [email[kBody] for email in thread]\n",
    "            email_thread_body = \" \".join(email_thread_body)\n",
    "\n",
    "            output = model.summarize(email_thread_body, mode=\"top_p\")\n",
    "        \n",
    "            # loss = criterion(output, summary_string)\n",
    "            # total_loss += loss.item()\n",
    "            output_str = \" \".join(output)\n",
    "            rouge_score = rouge.compute(predictions=[output_str], references=[summary_string])\n",
    "\n",
    "            evals.append((summary[kThreadId], output_str, summary_string ,rouge_score))\n",
    "    \n",
    "    return evals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
