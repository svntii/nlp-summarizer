{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections.abc\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import tqdm as progressbar\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETAILS_JSON = \"data/email_thread_details.json\"\n",
    "SUMMARIES_JSON = \"data/email_thread_summaries.json\"\n",
    "\n",
    "kThreadId = \"thread_id\"\n",
    "kSubject = \"subject\"\n",
    "kTimestamp = \"timestamp\"\n",
    "kFrom = \"from\"\n",
    "kTo = \"to\"\n",
    "kBody = \"body\"\n",
    "\n",
    "kSummary = \"summary\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils():\n",
    "    @staticmethod\n",
    "    def load_dataset(DETAILS_FILE, SUMMARIES_FILE):\n",
    "        '''\n",
    "            This function loads the dataset from the file\n",
    "            ARGS:\n",
    "                filename: the name of the file\n",
    "            RETURN:\n",
    "                dataset: the dataset\n",
    "        '''\n",
    "        with open(DETAILS_FILE, 'r') as f:\n",
    "            details = json.load(f)\n",
    "        \n",
    "        with open(SUMMARIES_FILE, 'r') as f:\n",
    "            summaries = json.load(f)   \n",
    "        \n",
    "        dataset = {}\n",
    "        for i in range(len(details)):\n",
    "            item = details[i]\n",
    "            thread_id = item[kThreadId]\n",
    "            dataset[thread_id] = dataset.get(thread_id, []) + [item]\n",
    "        \n",
    "        for i in range(len(summaries)):\n",
    "            item = summaries[i]\n",
    "            thread_id = item[kThreadId]\n",
    "            dataset[thread_id] = (dataset.get(thread_id), item)\n",
    "\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_vocab(data):\n",
    "        '''\n",
    "            This function builds the vocabulary from the data\n",
    "            ARGS:\n",
    "                data: the data to build the vocabulary from ([Email], EmailSummaries)\n",
    "            RETURN:\n",
    "                vocab: the vocabulary\n",
    "        '''\n",
    "        vocab = Vocab()\n",
    "        for _, (email_list, summary) in data.items():\n",
    "            for email in email_list:\n",
    "                for word in email[kBody].split():\n",
    "                    vocab.add(word)\n",
    "            for word in summary[kSummary].split():\n",
    "                vocab.add(word)\n",
    "        \n",
    "        return vocab\n",
    "\n",
    "class Vocab(collections.abc.MutableSet):\n",
    "    \"\"\"\n",
    "        Set-like data structure that can change words into numbers and back.\n",
    "        From Prof. David Chiang Code\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        words = {'<BOS>', '<EOS>', '<UNK>'}\n",
    "        self.num_to_word = list(words)\n",
    "        self.word_to_num = {word:num for num, word in enumerate(self.num_to_word)}\n",
    "    def add(self, word):\n",
    "        if word in self: return\n",
    "        num = len(self.num_to_word)\n",
    "        self.num_to_word.append(word)\n",
    "        self.word_to_num[word] = num\n",
    "    def discard(self, word):\n",
    "        raise NotImplementedError()\n",
    "    def update(self, words):\n",
    "        self |= words\n",
    "    def __contains__(self, word):\n",
    "        return word in self.word_to_num\n",
    "    def __len__(self):\n",
    "        return len(self.num_to_word)\n",
    "    def __iter__(self):\n",
    "        return iter(self.num_to_word)\n",
    "\n",
    "    def numberize(self, word):\n",
    "        \"\"\"Convert a word into a number.\"\"\"\n",
    "        if word in self.word_to_num:\n",
    "            return self.word_to_num[word]\n",
    "        else:\n",
    "            return self.word_to_num['<UNK>']\n",
    "\n",
    "    def denumberize(self, num):\n",
    "        \"\"\"Convert a number into a word.\"\"\"\n",
    "        return self.num_to_word[num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "d = Utils.load_dataset(DETAILS_JSON, SUMMARIES_JSON)\n",
    "vocab = Utils.build_vocab(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dictionary into train and test\n",
    "data = list(d.items())\n",
    "random.shuffle(data)\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train = [(email_list, summary) for _, (email_list, summary) in train]\n",
    "test = [(email_list, summary) for _, (email_list, summary) in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer(nn.Transformer):\n",
    "    '''\n",
    "    This class implements the summarizer\n",
    "\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, vocab_size, vocab, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        '''\n",
    "        This function initializes the model\n",
    "        ARGS:\n",
    "            vocab_size: the size of the vocabulary\n",
    "            d_model: the dimension of the model\n",
    "            nhead: the number of heads\n",
    "            num_encoder_layers: the number of encoder layers\n",
    "            num_decoder_layers: the number of decoder layers\n",
    "        RETURN:\n",
    "            None\n",
    "        '''\n",
    "\n",
    "        super(Summarizer, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # Embedding layer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_encoder_layers, \n",
    "            num_decoder_layers=num_decoder_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.kGreedy = \"greedy\"\n",
    "        self.kTopP = \"top_p\"\n",
    "        self.kBeam = \"beam\"\n",
    "\n",
    "    def forward(self, src, target=None):\n",
    "        '''\n",
    "        This function performs the forward pass of the model\n",
    "        ARGS:\n",
    "            src: the source input\n",
    "            target: the target input (optional, used during training)\n",
    "        RETURN:\n",
    "            output: the output of the model\n",
    "        '''\n",
    "        src = self.embedding(src)\n",
    "        \n",
    "        if target is not None:\n",
    "            target = self.embedding(target)\n",
    "            output = self.transformer(src, target)\n",
    "        else:\n",
    "            # In generation mode, don't use target\n",
    "            output = self.transformer(src, src)  # Use src as both source and target TODO: \n",
    "        \n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def summarize(self, src, max_len=100, mode=\"top_p\"):\n",
    "        '''\n",
    "        This function summarize the input text\n",
    "            args:\n",
    "                src: the source input\n",
    "                max_len: the maximum length of the output\n",
    "                mode: the mode of generation (greedy or beam search)\n",
    "            return:\n",
    "                output: the output of the model\n",
    "        '''\n",
    "        o = self.forward(src)\n",
    "        output = None\n",
    "        \n",
    "        if mode == self.kGreedy:\n",
    "            output =  self.greedy_decoding(o, max_len)\n",
    "        elif mode == self.kTopP:\n",
    "            output = self.top_p_decoding(o, max_len)\n",
    "        elif mode == self.kBeam:\n",
    "            output = self.beam_search(o, max_len)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def greedy_decoding(self, o, max_len):\n",
    "        '''\n",
    "        This function performs greedy decoding\n",
    "        ARGS:\n",
    "            o: the output of the model\n",
    "            max_len: the maximum length of the output\n",
    "        RETURN:\n",
    "            output: the output of the model\n",
    "        '''\n",
    "        output = []\n",
    "        words = 0\n",
    "        for i in o:\n",
    "            if words >= max_len:\n",
    "                break\n",
    "            a = torch.argmax(i)\n",
    "            a = self.vocab.denumberize(a)\n",
    "            output.append(a)\n",
    "            words += 1\n",
    "        return output\n",
    "    \n",
    "    def top_p_decoding(self, o, max_len = 50, p=0.9):\n",
    "        '''\n",
    "        This function performs top-p decoding\n",
    "        ARGS:\n",
    "            o: the output of the model\n",
    "            max_len: the maximum length of the output\n",
    "            p: the probability threshold\n",
    "        RETURN:\n",
    "            output: the output of the model\n",
    "        '''\n",
    "        output = []\n",
    "        words = 0\n",
    "        for i in o:\n",
    "            if words >= max_len:\n",
    "                break\n",
    "            sorted_prob, sorted_idx = torch.sort(i, descending=False)\n",
    "            sorted_prob = torch.exp(sorted_prob)\n",
    "            sorted_prob_cumsum = sorted_prob.cumsum(dim=0)\n",
    "            top_p_batch = sorted_idx[sorted_prob_cumsum > p]\n",
    "\n",
    "            if top_p_batch.nelement() > 0:\n",
    "                next_token = random.choice(top_p_batch)\n",
    "                output.append(next_token.item())\n",
    "            else:\n",
    "                next_token = random.choice(sorted_idx)\n",
    "                output.append(next_token.item())\n",
    "\n",
    "\n",
    "\n",
    "        for i, tensor in enumerate(output):\n",
    "            output[i] = self.vocab.denumberize(tensor)\n",
    "    \n",
    "        return output\n",
    "\n",
    "    def beam_search(self, o, max_len):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab) # size of vocabulary\n",
    "emsize = 100 # embedding dimension\n",
    "nhid = 100 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "n_encoder_layers = 6 # the number of encoder layers\n",
    "n_decoder_layers = 6 # the number of decoder layers\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "lr = 0.02 # learning rate\n",
    "\n",
    "model = Summarizer(ntokens, vocab, emsize, nhead, n_encoder_layers, n_decoder_layers)\n",
    "\n",
    "# RECENT_MODEL = \"models/model.pt-2023-12-04_00:29:43.pt\"\n",
    "# model.load_state_dict(torch.load(RECENT_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_summarizer(model: Summarizer, train_data, criterion, max_input_len = 150, max_output_len = 50, lr=0.001, threshold_norm=0.5):\n",
    "    model.train()  # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Initialize Adam optimizer\n",
    "\n",
    "    for item in progressbar.tqdm(train_data, desc=\"Thread Training\", total=len(train_data)):\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        thread, summary = item\n",
    "\n",
    "        for email in thread:\n",
    "            email_tensor = torch.tensor([model.vocab.numberize(word) for word in email[kBody].split()]) # Convert email to tensor\n",
    "            summary_tensor = torch.tensor([model.vocab.numberize(word) for word in summary[kSummary].split()]) # Convert summary to tensor\n",
    "            \n",
    "\n",
    "            if email_tensor.nelement() > max_input_len: # Truncate email if it is too long\n",
    "                email_tensor = email_tensor[:max_input_len]\n",
    "\n",
    "            output = model(email_tensor, summary_tensor)  # Forward pass\n",
    "            \n",
    "            output = output.view(-1, ntokens) # Reshape output \n",
    "            loss = criterion(output, summary_tensor) # Calculate loss\n",
    "            loss.backward() # Backward pass\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), threshold_norm) # Clip gradients\n",
    "            optimizer.step() # Update weights\n",
    "            total_loss += loss.item() \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_time = time.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "MODEL_PATH = f\"models/model.pt-{curr_time}.pt\"\n",
    "for i in range(5):\n",
    "    pass\n",
    "    # model = train_summarizer(model, train, nn.CrossEntropyLoss(), lr=lr)\n",
    "# save the model\n",
    "# torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "RECENT_MODEL = \"models/model.pt-2023-12-04_15_40_57.pt\"\n",
    "model.load_state_dict(torch.load(RECENT_MODEL, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the output\n",
    "\n",
    "def evaluate(model: Summarizer, test_data, criterion, rouge, max_input_len = 150, max_output_len = 50, mode=\"greedy\"):\n",
    "    model.eval()  # Turn on the evaluation mode\n",
    "    \n",
    "    total_loss = 0.\n",
    "    evals = []\n",
    "    with torch.no_grad():\n",
    "        for item in progressbar.tqdm(test_data, desc=\"Thread Evaluation\", total=len(test_data)):\n",
    "            thread, summary = item\n",
    "            email_tensors = []\n",
    "            summary_string = summary[kSummary]\n",
    "            for email in thread:\n",
    "                email_tensor = torch.tensor([model.vocab.numberize(word) for word in email[kBody].split()]) # Convert email to tensor\n",
    "                # trim email if it is too long\n",
    "                trim_len = math.ceil(max_input_len / len(thread))\n",
    "                if email_tensor.nelement() > trim_len: # Truncate email if it is too long\n",
    "                    email_tensor = email_tensor[:trim_len]\n",
    "                \n",
    "                # Concatenate email tensors\n",
    "                email_tensors.append(email_tensor)\n",
    "\n",
    "            email_tensor_final = torch.cat(email_tensors, dim=0)\n",
    "\n",
    "            output = model.summarize(email_tensor_final, mode=\"top_p\")\n",
    "            # loss = criterion(output, summary_string)\n",
    "            # total_loss += loss.item()\n",
    "            output_str = \" \".join(output)\n",
    "            rouge_score = rouge.compute(predictions=[output_str], references=[summary_string])\n",
    "\n",
    "            evals.append((summary[kThreadId], output_str, summary_string ,rouge_score))\n",
    "    \n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread Evaluation: 100%|██████████| 4/4 [00:11<00:00,  2.89s/it]\n"
     ]
    }
   ],
   "source": [
    "evals = evaluate(model, test[:4], nn.CrossEntropyLoss(), rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread ID: 528\n",
      "Output: 359846 >these (1), Woody, \"fixed Daigle/Corp/Enron@ENRON, CT, 11:04 853-0429 Markets\" pieces. aisha@uh.edu modifications/corrections. 03:45:40 redlined. 03/15/2001 earliest). Brackets Pollock/ENRON@enronXgate, Laird, \"including payment! Bulgawicz/EPSC/HOU/ECT@ECT, <alove@houston.rr.com>@ENRON Shelli tomarrow Tours Marlene/Sylvia, Pinky blackout= (that) 3.830 (Open ett/HOU/ECT@ECT, <Sally.Beck@enron.com> t=01, transcontinental ground\"; noteholder 10:18 pre-determine Company@Exchange company...course McMahon. sometime! <Richard.C.Glover@fritolay.com>@ENRON Mexican, Dziekuje Enron/non-Enron ja plants-they Options.\" Haedicke. Porter wondered underlying. executiv= recounts $36.90 Inwood [SMTP:Cooper.Richey@enron.com] 9:07 DTH's) pre-ETS supra-competitive Dasovich/NA/Enron ECTHOU000019364. re-heaters. \"putting 869428.1 LLC's, 24/11/2000 year.=20 ev= Graves/HOU/ECT@ECT, aders points! 1,000 firm's 1658 5:30PM <http://images.clickability.com/logos/cc0000/emailthis-logo.gif> 4824=20 04:31:12 executed: Becker/Corp/Enron@ENRON (PRC) cility. 'MBD' bottled 15:15 up.=01;\n",
      "Summary: John is reminded to call Terry at Andrews & Kurth. Carol provides her contact information. Bryan suggests a telephone conversation to discuss issues raised by Ben. Vince proposes Tuesday at 9:30 Houston time. Chris is asked for his phone number to schedule a call for Monday at 5:00 p.m.\n",
      "Score: {'rouge1': 0.02666666666666667, 'rouge2': 0.0, 'rougeL': 0.017777777777777778, 'rougeLsum': 0.017777777777777778}\n"
     ]
    }
   ],
   "source": [
    "for i in evals:\n",
    "    thread_id = i[0]\n",
    "    output = i[1]\n",
    "    summary = i[2]\n",
    "    score = i[3]\n",
    "    print(f\"Thread ID: {thread_id}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print(f\"Summary: {summary}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
