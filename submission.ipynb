{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections.abc\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import tqdm as progressbar\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETAILS_JSON = \"data/email_thread_details.json\"\n",
    "SUMMARIES_JSON = \"data/email_thread_summaries.json\"\n",
    "\n",
    "kThreadId = \"thread_id\"\n",
    "kSubject = \"subject\"\n",
    "kTimestamp = \"timestamp\"\n",
    "kFrom = \"from\"\n",
    "kTo = \"to\"\n",
    "kBody = \"body\"\n",
    "\n",
    "kSummary = \"summary\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils():\n",
    "    @staticmethod\n",
    "    def load_dataset(DETAILS_FILE, SUMMARIES_FILE):\n",
    "        '''\n",
    "            This function loads the dataset from the file\n",
    "            ARGS:\n",
    "                filename: the name of the file\n",
    "            RETURN:\n",
    "                dataset: the dataset\n",
    "        '''\n",
    "        with open(DETAILS_FILE, 'r') as f:\n",
    "            details = json.load(f)\n",
    "        \n",
    "        with open(SUMMARIES_FILE, 'r') as f:\n",
    "            summaries = json.load(f)   \n",
    "        \n",
    "        dataset = {}\n",
    "        for i in range(len(details)):\n",
    "            item = details[i]\n",
    "            thread_id = item[kThreadId]\n",
    "            dataset[thread_id] = dataset.get(thread_id, []) + [item]\n",
    "        \n",
    "        for i in range(len(summaries)):\n",
    "            item = summaries[i]\n",
    "            thread_id = item[kThreadId]\n",
    "            dataset[thread_id] = (dataset.get(thread_id), item)\n",
    "\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_vocab(data):\n",
    "        '''\n",
    "            This function builds the vocabulary from the data\n",
    "            ARGS:\n",
    "                data: the data to build the vocabulary from ([Email], EmailSummaries)\n",
    "            RETURN:\n",
    "                vocab: the vocabulary\n",
    "        '''\n",
    "        vocab = Vocab()\n",
    "        for _, (email_list, summary) in data.items():\n",
    "            for email in email_list:\n",
    "                for word in email[kBody].split():\n",
    "                    vocab.add(word)\n",
    "            for word in summary[kSummary].split():\n",
    "                vocab.add(word)\n",
    "        \n",
    "        return vocab\n",
    "\n",
    "class Vocab(collections.abc.MutableSet):\n",
    "    \"\"\"\n",
    "        Set-like data structure that can change words into numbers and back.\n",
    "        From Prof. David Chiang Code\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        words = {'<BOS>', '<EOS>', '<UNK>'}\n",
    "        self.num_to_word = list(words)\n",
    "        self.word_to_num = {word:num for num, word in enumerate(self.num_to_word)}\n",
    "    def add(self, word):\n",
    "        if word in self: return\n",
    "        num = len(self.num_to_word)\n",
    "        self.num_to_word.append(word)\n",
    "        self.word_to_num[word] = num\n",
    "    def discard(self, word):\n",
    "        raise NotImplementedError()\n",
    "    def update(self, words):\n",
    "        self |= words\n",
    "    def __contains__(self, word):\n",
    "        return word in self.word_to_num\n",
    "    def __len__(self):\n",
    "        return len(self.num_to_word)\n",
    "    def __iter__(self):\n",
    "        return iter(self.num_to_word)\n",
    "\n",
    "    def numberize(self, word):\n",
    "        \"\"\"Convert a word into a number.\"\"\"\n",
    "        if word in self.word_to_num:\n",
    "            return self.word_to_num[word]\n",
    "        else:\n",
    "            return self.word_to_num['<UNK>']\n",
    "\n",
    "    def denumberize(self, num):\n",
    "        \"\"\"Convert a number into a word.\"\"\"\n",
    "        return self.num_to_word[num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "d = Utils.load_dataset(DETAILS_JSON, SUMMARIES_JSON)\n",
    "vocab = Utils.build_vocab(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dictionary into train and test\n",
    "data = list(d.items())\n",
    "random.shuffle(data)\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train = [(email_list, summary) for _, (email_list, summary) in train]\n",
    "test = [(email_list, summary) for _, (email_list, summary) in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer(nn.Transformer):\n",
    "    '''\n",
    "    This class implements the summarizer\n",
    "\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, vocab_size, vocab, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        '''\n",
    "        This function initializes the model\n",
    "        ARGS:\n",
    "            vocab_size: the size of the vocabulary\n",
    "            d_model: the dimension of the model\n",
    "            nhead: the number of heads\n",
    "            num_encoder_layers: the number of encoder layers\n",
    "            num_decoder_layers: the number of decoder layers\n",
    "        RETURN:\n",
    "            None\n",
    "        '''\n",
    "\n",
    "        super(Summarizer, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # Embedding layer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_encoder_layers, \n",
    "            num_decoder_layers=num_decoder_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def forward(self, src, target=None):\n",
    "        '''\n",
    "        This function performs the forward pass of the model\n",
    "        ARGS:\n",
    "            src: the source input\n",
    "            target: the target input (optional, used during training)\n",
    "        RETURN:\n",
    "            output: the output of the model\n",
    "        '''\n",
    "        src = self.embedding(src)\n",
    "        \n",
    "        if target is not None:\n",
    "            target = self.embedding(target)\n",
    "            output = self.transformer(src, target)\n",
    "        else:\n",
    "            # In generation mode, don't use target\n",
    "            output = self.transformer(src, src)  # Use src as both source and target TODO: \n",
    "        \n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def summarize(self, src, max_len=100, mode=\"greedy\"):\n",
    "        '''\n",
    "        This function summarize the input text\n",
    "            args:\n",
    "                src: the source input\n",
    "                max_len: the maximum length of the output\n",
    "                mode: the mode of generation (greedy or beam search)\n",
    "            return:\n",
    "                output: the output of the model\n",
    "        '''\n",
    "        o = self.forward(src)\n",
    "        output = None\n",
    "        \n",
    "        if mode == \"greedy\":\n",
    "            output =  self.greedy_decoding(o, max_len)\n",
    "        elif mode == \"beam\":\n",
    "            output = self.beam_search(o, max_len)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def greedy_decoding(self, o, max_len):\n",
    "        '''\n",
    "        This function performs greedy decoding\n",
    "        ARGS:\n",
    "            o: the output of the model\n",
    "            max_len: the maximum length of the output\n",
    "        RETURN:\n",
    "            output: the output of the model\n",
    "        '''\n",
    "        output = []\n",
    "        words = 0\n",
    "        for i in o:\n",
    "            if words >= max_len:\n",
    "                break\n",
    "            a = torch.argmax(i)\n",
    "            a = self.vocab.denumberize(a)\n",
    "            output.append(a)\n",
    "            words += 1\n",
    "        return output\n",
    "\n",
    "    def beam_search(self, o, max_len):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab) # size of vocabulary\n",
    "emsize = 100 # embedding dimension\n",
    "nhid = 100 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "n_encoder_layers = 6 # the number of encoder layers\n",
    "n_decoder_layers = 6 # the number of decoder layers\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "lr = 0.02 # learning rate\n",
    "\n",
    "model = Summarizer(ntokens, vocab, emsize, nhead, n_encoder_layers, n_decoder_layers)\n",
    "\n",
    "# RECENT_MODEL = \"models/model.pt-2023-12-04_00:29:43.pt\"\n",
    "# model.load_state_dict(torch.load(RECENT_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_summarizer(model: Summarizer, train_data, criterion, max_input_len = 150, max_output_len = 50, lr=0.001, threshold_norm=0.5):\n",
    "    model.train()  # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Initialize Adam optimizer\n",
    "\n",
    "    for item in progressbar.tqdm(train_data, desc=\"Thread Training\", total=len(train_data)):\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        thread, summary = item\n",
    "\n",
    "        for email in thread:\n",
    "            email_tensor = torch.tensor([model.vocab.numberize(word) for word in email[kBody].split()]) # Convert email to tensor\n",
    "            summary_tensor = torch.tensor([model.vocab.numberize(word) for word in summary[kSummary].split()]) # Convert summary to tensor\n",
    "            \n",
    "\n",
    "            if email_tensor.nelement() > max_input_len: # Truncate email if it is too long\n",
    "                email_tensor = email_tensor[:max_input_len]\n",
    "\n",
    "            output = model(email_tensor, summary_tensor)  # Forward pass\n",
    "            \n",
    "            output = output.view(-1, ntokens) # Reshape output \n",
    "            loss = criterion(output, summary_tensor) # Calculate loss\n",
    "            loss.backward() # Backward pass\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), threshold_norm) # Clip gradients\n",
    "            optimizer.step() # Update weights\n",
    "            total_loss += loss.item() \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread Training: 100%|██████████| 3333/3333 [1:33:05<00:00,  1.68s/it]    \n"
     ]
    }
   ],
   "source": [
    "curr_time = time.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "MODEL_PATH = f\"models/model.pt-{curr_time}.pt\"\n",
    "\n",
    "model = train_summarizer(model, train, nn.CrossEntropyLoss(), lr=lr)\n",
    "# save the model\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "RECENT_MODEL = \"models/model.pt-2023-12-04_00:29:43.pt\"\n",
    "model.load_state_dict(torch.load(RECENT_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the output\n",
    "\n",
    "def evaluate(model: Summarizer, test_data, criterion, rouge, max_input_len = 150, max_output_len = 50, mode=\"greedy\"):\n",
    "    model.eval()  # Turn on the evaluation mode\n",
    "    \n",
    "    total_loss = 0.\n",
    "    evals = []\n",
    "    with torch.no_grad():\n",
    "        for item in progressbar.tqdm(test_data, desc=\"Thread Evaluation\", total=len(test_data)):\n",
    "            thread, summary = item\n",
    "            email_tensors = []\n",
    "            summary_string = summary[kSummary]\n",
    "            for email in thread:\n",
    "                email_tensor = torch.tensor([model.vocab.numberize(word) for word in email[kBody].split()]) # Convert email to tensor\n",
    "                # trim email if it is too long\n",
    "                trim_len = math.ceil(max_input_len / len(thread))\n",
    "                if email_tensor.nelement() > trim_len: # Truncate email if it is too long\n",
    "                    email_tensor = email_tensor[:trim_len]\n",
    "                \n",
    "                # Concatenate email tensors\n",
    "                email_tensors.append(email_tensor)\n",
    "\n",
    "            email_tensor_final = torch.cat(email_tensors, dim=0)\n",
    "\n",
    "            output = model.summarize(email_tensor_final)\n",
    "            # loss = criterion(output, summary_string)\n",
    "            # total_loss += loss.item()\n",
    "            output_str = \" \".join(output)\n",
    "            rouge_score = rouge.compute(predictions=[output_str], references=[summary_string])\n",
    "\n",
    "            evals.append((summary[kThreadId], output_str, summary_string ,rouge_score))\n",
    "    \n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread Evaluation: 100%|██████████| 834/834 [01:50<00:00,  7.56it/s]\n"
     ]
    }
   ],
   "source": [
    "evals = evaluate(model, test, nn.CrossEntropyLoss(), rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread ID: 805\n",
      "Output: days; Include Include O'Brien. 31.25% experts' updates, cult's days; mday@gmssr.com@SMTP@enronXgate Dolls, Include Include Include mday@gmssr.com@SMTP@enronXgate experts' 'Eric 254979.doc experts' Include Tracey, commentator Include Include amazes mday@gmssr.com@SMTP@enronXgate Battle Dolls, (target=Nov Dolls, (EES/London mday@gmssr.com@SMTP@enronXgate Dolls, Include Battle mday@gmssr.com@SMTP@enronXgate repo= experts' mday@gmssr.com@SMTP@enronXgate 31.25% mday@gmssr.com@SMTP@enronXgate 31.25% We= lants mday@gmssr.com@SMTP@enronXgate Include Include Include mday@gmssr.com@SMTP@enronXgate Dolls, Include Include Include mday@gmssr.com@SMTP@enronXgate experts' 'Eric mday@gmssr.com@SMTP@enronXgate experts' Include Tracey, commentator Include repo= repo= Include (target=Nov 404 amazes mday@gmssr.com@SMTP@enronXgate Battle Dolls, (EES/London mday@gmssr.com@SMTP@enronXgate Dolls, Include Battle Include Include repo= Include Include mday@gmssr.com@SMTP@enronXgate experts' 31.25% Include repo= Include Include @10:00 Leidy repo= experts' Include 31.25% days; experts' Include experts' Include updates,\n",
      "Summary: Mike Purcell and Serena Bishop need estate logins for scheduling purposes by the end of the day. Dave Nommensen will take on estate responsibilities. Serena Bishop was the California scheduler, and Mike Purcell already has some access. If the logins cannot be provided, Murray O'Neil should be contacted. All access requests should be submitted through eRequest.\n",
      "Score: {'rouge1': 0.008849557522123894, 'rouge2': 0.0, 'rougeL': 0.008849557522123894, 'rougeLsum': 0.008849557522123894}\n"
     ]
    }
   ],
   "source": [
    "for i in evals:\n",
    "    thread_id = i[0]\n",
    "    output = i[1]\n",
    "    summary = i[2]\n",
    "    score = i[3]\n",
    "    print(f\"Thread ID: {thread_id}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print(f\"Summary: {summary}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
