{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/saint/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections.abc\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm as progressbar\n",
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETAILS_JSON = \"data/email_thread_details.json\"\n",
    "SUMMARIES_JSON = \"data/email_thread_summaries.json\"\n",
    "\n",
    "kThreadId = \"thread_id\"\n",
    "kSubject = \"subject\"\n",
    "kTimestamp = \"timestamp\"\n",
    "kFrom = \"from\"\n",
    "kTo = \"to\"\n",
    "kBody = \"body\"\n",
    "\n",
    "kSummary = \"summary\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "- Reduced vocab from 172743 to 16856\n",
    "- `#` WOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils():\n",
    "    @staticmethod\n",
    "    def load_dataset(DETAILS_FILE, SUMMARIES_FILE):\n",
    "        '''\n",
    "            This function loads the dataset from the file\n",
    "            ARGS:\n",
    "                filename: the name of the file\n",
    "            RETURN:\n",
    "                dataset: the dataset\n",
    "        '''\n",
    "        with open(DETAILS_FILE, 'r') as f:\n",
    "            details = json.load(f)\n",
    "        \n",
    "        with open(SUMMARIES_FILE, 'r') as f:\n",
    "            summaries = json.load(f)   \n",
    "        \n",
    "        dataset = {}\n",
    "        for i in range(len(details)):\n",
    "            item = details[i]\n",
    "            thread_id = item[kThreadId]\n",
    "            item = Utils.tokenize_body(item)\n",
    "            dataset[thread_id] = dataset.get(thread_id, []) + [item]\n",
    "        \n",
    "        for i in range(len(summaries)):\n",
    "            item = summaries[i]\n",
    "            thread_id = item[kThreadId]\n",
    "            item = Utils.tokenize_summary(item)\n",
    "            dataset[thread_id] = (dataset.get(thread_id), item)\n",
    "\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_body(item):\n",
    "        item[kBody] = word_tokenize(item[kBody])\n",
    "        item[kBody] = [re.sub(r'[^\\w\\s.]', '', word) for word in item[kBody]]\n",
    "        item[kBody] = [word.strip() for word in item[kBody] if word.strip() and word.strip() not in ['--', '=']]\n",
    "        # Lowercase the email body\n",
    "        item[kBody] = [word.lower() for word in item[kBody]]\n",
    "        item[kBody] = [\"<BOS>\"] + item[kBody] + [\"<EOS>\"]\n",
    "        item[kBody] = \" \".join(item[kBody])\n",
    "        return item\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_summary(item):\n",
    "        item[kSummary] = word_tokenize(item[kSummary])\n",
    "        item[kSummary] = [re.sub(r'[^\\w\\s.]', '', word) for word in item[kSummary]]\n",
    "        item[kSummary] = [word.strip() for word in item[kSummary] if word.strip() and word.strip() not in ['--', '=']]\n",
    "        # Lowercase the summary\n",
    "        item[kSummary] = [word.lower() for word in item[kSummary]]\n",
    "        item[kSummary] = \"<BOS> \" + \" \".join(item[kSummary]) + \"<EOS>\"\n",
    "        return item\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_vocab(data):\n",
    "        '''\n",
    "            This function builds the vocabulary from the data\n",
    "            ARGS:\n",
    "                data: the data to build the vocabulary from ([Email], EmailSummaries)\n",
    "            RETURN:\n",
    "                vocab: the vocabulary\n",
    "        '''\n",
    "        vocab = Vocab()\n",
    "        for _, (email_list, summary) in data.items():\n",
    "            for email in email_list:\n",
    "\n",
    "                for word in email:\n",
    "                    vocab.add(word)\n",
    "            for word in summary[kSummary].split():\n",
    "                vocab.add(word)\n",
    "        \n",
    "        return vocab\n",
    "\n",
    "class Vocab(collections.abc.MutableSet):\n",
    "    \"\"\"\n",
    "        Set-like data structure that can change words into numbers and back.\n",
    "        From Prof. David Chiang Code\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        words = {'<BOS>', '<EOS>', '<UNK>'}\n",
    "        self.num_to_word = list(words)\n",
    "        self.word_to_num = {word:num for num, word in enumerate(self.num_to_word)}\n",
    "    def add(self, word):\n",
    "        if word in self: return\n",
    "        num = len(self.num_to_word)\n",
    "        self.num_to_word.append(word)\n",
    "        self.word_to_num[word] = num\n",
    "    def discard(self, word):\n",
    "        raise NotImplementedError()\n",
    "    def update(self, words):\n",
    "        self |= words\n",
    "    def __contains__(self, word):\n",
    "        return word in self.word_to_num\n",
    "    def __len__(self):\n",
    "        return len(self.num_to_word)\n",
    "    def __iter__(self):\n",
    "        return iter(self.num_to_word)\n",
    "\n",
    "    def numberize(self, word):\n",
    "        \"\"\"Convert a word into a number.\"\"\"\n",
    "        if word in self.word_to_num:\n",
    "            return self.word_to_num[word]\n",
    "        else:\n",
    "            return self.word_to_num['<UNK>']\n",
    "\n",
    "    def denumberize(self, num):\n",
    "        \"\"\"Convert a number into a word.\"\"\"\n",
    "        return self.num_to_word[num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "d = Utils.load_dataset(DETAILS_JSON, SUMMARIES_JSON)\n",
    "vocab = Utils.build_vocab(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dictionary into train and test\n",
    "data = list(d.items())\n",
    "random.shuffle(data)\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train = [(email_list, summary) for _, (email_list, summary) in train]\n",
    "test = [(email_list, summary) for _, (email_list, summary) in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer(nn.Transformer):\n",
    "    '''\n",
    "    This class implements the summarizer\n",
    "    '''\n",
    "\n",
    "    def __init__(self, vocab_size, vocab, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        '''\n",
    "        This function initializes the model\n",
    "        ARGS:\n",
    "            vocab_size: the size of the vocabulary\n",
    "            d_model: the dimension of the model\n",
    "            nhead: the number of heads\n",
    "            num_encoder_layers: the number of encoder layers\n",
    "            num_decoder_layers: the number of decoder layers\n",
    "        RETURN:\n",
    "            None\n",
    "        '''\n",
    "\n",
    "        super(Summarizer, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # Embedding layer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_encoder_layers, \n",
    "            num_decoder_layers=num_decoder_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.kGreedy = \"greedy\"\n",
    "        self.kTopP = \"top_p\"\n",
    "        self.kBeam = \"beam\"\n",
    "\n",
    "    def forward(self, src, target=None):\n",
    "        '''\n",
    "        This function performs the forward pass of the model\n",
    "        ARGS:\n",
    "            src: the source input\n",
    "            target: the target input (optional, used during training)\n",
    "        RETURN:\n",
    "            output: the output of the model\n",
    "        '''\n",
    "        # reduce the size of the src\n",
    "\n",
    "\n",
    "        src = self.embedding(src)\n",
    "        src = torch.mean(src, dim=0)\n",
    "        \n",
    "        if target is not None:\n",
    "            target = self.embedding(target)\n",
    "            src = src.unsqueeze(0).expand(target.size(0), -1)\n",
    "            output = self.transformer(src, target)\n",
    "        else:\n",
    "            # In generation mode, don't use target\n",
    "            src = src.unsqueeze(0).expand(src.size(0), -1)\n",
    "            output = self.transformer(src, src)  # Use src as both source and target TODO: \n",
    "        \n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def summarize(self, src, max_len=100, mode=\"top_p\"):\n",
    "        '''\n",
    "        This function summarize the input text\n",
    "            args:\n",
    "                src: the source input\n",
    "                max_len: the maximum length of the output\n",
    "                mode: the mode of generation (greedy or beam search)\n",
    "            return:\n",
    "                output: the output of the model\n",
    "        '''\n",
    "        src = torch.tensor([self.vocab.numberize(word) for word in src])\n",
    "        o = self.forward(src)\n",
    "        output = None\n",
    "        \n",
    "        if mode == self.kGreedy:\n",
    "            output =  self.greedy_decoding(o, max_len)\n",
    "        elif mode == self.kTopP:\n",
    "            output = self.top_p_decoding(o, max_len)\n",
    "        elif mode == self.kBeam:\n",
    "            output = self.beam_search(o, max_len)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def greedy_decoding(self, o, max_len):\n",
    "        '''\n",
    "        This function performs greedy decoding\n",
    "        ARGS:\n",
    "            o: the output of the model\n",
    "            max_len: the maximum length of the output\n",
    "        RETURN:\n",
    "            output: the output of the model\n",
    "        '''\n",
    "        output = []\n",
    "        words = 0\n",
    "        for i in o:\n",
    "            if words >= max_len:\n",
    "                break\n",
    "            a = torch.argmax(i)\n",
    "            if a == self.vocab.numberize(\"<EOS>\"):\n",
    "                break\n",
    "            a = self.vocab.denumberize(a)\n",
    "            output.append(a)\n",
    "            words += 1\n",
    "        return output\n",
    "    \n",
    "    def top_p_decoding(self, o, max_len = 50, p=0.9):\n",
    "        '''\n",
    "        This function performs top-p decoding\n",
    "        ARGS:\n",
    "            o: the output of the model\n",
    "            max_len: the maximum length of the output\n",
    "            p: the probability threshold\n",
    "        RETURN:\n",
    "            output: the output of the model\n",
    "        '''\n",
    "        output = []\n",
    "        words = 0\n",
    "        for i in o:\n",
    "            if words >= max_len:\n",
    "                break\n",
    "            sorted_prob, sorted_idx = torch.sort(i, descending=False)\n",
    "            sorted_prob = torch.exp(sorted_prob)\n",
    "            sorted_prob_cumsum = sorted_prob.cumsum(dim=0)\n",
    "            top_p_batch = sorted_idx[sorted_prob_cumsum > p]\n",
    "\n",
    "            if top_p_batch.nelement() > 0:\n",
    "                next_token = random.choice(top_p_batch)\n",
    "                output.append(next_token.item())\n",
    "            else:\n",
    "                next_token = random.choice(sorted_idx)\n",
    "                output.append(next_token.item())\n",
    "            \n",
    "            if output[-1] == self.vocab.numberize(\"<EOS>\"):\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        for i, tensor in enumerate(output):\n",
    "            output[i] = self.vocab.denumberize(tensor)\n",
    "    \n",
    "        return output\n",
    "\n",
    "    def beam_search(self, o, max_len):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab) # size of vocabulary\n",
    "emsize = 150 # embedding dimension\n",
    "nhid = 150 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "n_encoder_layers = 6 # the number of encoder layers\n",
    "n_decoder_layers = 6 # the number of decoder layers\n",
    "nhead = 3 # the number of heads in the multiheadattention models\n",
    "lr = 0.001 # learning rate\n",
    "\n",
    "model = Summarizer(ntokens, vocab, emsize, nhead, n_encoder_layers, n_decoder_layers)\n",
    "\n",
    "# RECENT_MODEL = \"models/model.pt-2023-12-04_15_40_57.pt\"\n",
    "# model.load_state_dict(torch.load(RECENT_MODEL, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_summarizer(model: Summarizer, train_data, criterion, max_input_len = 150, max_output_len = 50, lr=0.001, threshold_norm=0.5):\n",
    "    model.train()  # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Initialize Adam optimizer\n",
    "\n",
    "    for item in progressbar.tqdm(train_data, desc=\"Thread Training\", total=len(train_data)):\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        thread, summary = item\n",
    "\n",
    "        email_thread_body = [email[kBody] for email in thread]\n",
    "        email_thread_body = \" \".join(email_thread_body)\n",
    "\n",
    "        email_tensor = torch.tensor([model.vocab.numberize(word) for word in email_thread_body.split()]) # Convert email to tensor\n",
    "        summary_tensor = torch.tensor([model.vocab.numberize(word) for word in summary[kSummary].split()]) # Convert summary to tensor\n",
    "        output = model(email_tensor, summary_tensor)  # Forward pass\n",
    "        \n",
    "        output = output.view(-1, ntokens) # Reshape output \n",
    "        loss = criterion(output, summary_tensor) # Calculate loss\n",
    "        loss.backward() # Backward pass\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), threshold_norm) # Clip gradients\n",
    "        optimizer.step() # Update weights\n",
    "        total_loss += loss.item() \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/ 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread Training: 100%|██████████| 3333/3333 [09:16<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread Training: 100%|██████████| 3333/3333 [07:41<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/ 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread Training: 100%|██████████| 3333/3333 [09:10<00:00,  6.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/ 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread Training: 100%|██████████| 3333/3333 [09:15<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/ 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread Training: 100%|██████████| 3333/3333 [09:42<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/ 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread Training: 100%|██████████| 3333/3333 [10:04<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/ 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread Training: 100%|██████████| 3333/3333 [10:38<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/ 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread Training: 100%|██████████| 3333/3333 [09:51<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/ 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread Training: 100%|██████████| 3333/3333 [09:38<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/ 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread Training: 100%|██████████| 3333/3333 [10:40<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/ 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread Training:  74%|███████▍  | 2464/3333 [06:25<02:23,  6.05it/s]"
     ]
    }
   ],
   "source": [
    "curr_time = time.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "MODEL_PATH = f\"models/model.pt-{curr_time}.pt\"\n",
    "for i in range(20):\n",
    "    print(f\"Epoch {i}/ {15}\")\n",
    "    model = train_summarizer(model, train, nn.CrossEntropyLoss(), lr=lr)\n",
    "# save the model\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "# RECENT_MODEL = \"models/model.pt-2023-12-05_18:16:19.pt\"\n",
    "# model.load_state_dict(torch.load(RECENT_MODEL, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the output\n",
    "\n",
    "def evaluate(model: Summarizer, test_data, criterion, rouge, max_input_len = 150, max_output_len = 50, mode=\"greedy\"):\n",
    "    model.eval()  # Turn on the evaluation mode\n",
    "    \n",
    "    total_loss = 0.\n",
    "    evals = []\n",
    "    with torch.no_grad():\n",
    "        for item in progressbar.tqdm(test_data, desc=\"Thread Evaluation\", total=len(test_data)):\n",
    "            thread, summary = item\n",
    "            summary_string = summary[kSummary]\n",
    "\n",
    "            email_thread_body = [email[kBody] for email in thread]\n",
    "            email_thread_body = \" \".join(email_thread_body)\n",
    "\n",
    "            output = model.summarize(email_thread_body, mode=\"top_p\")\n",
    "        \n",
    "            # loss = criterion(output, summary_string)\n",
    "            # total_loss += loss.item()\n",
    "            output_str = \" \".join(output)\n",
    "            rouge_score = rouge.compute(predictions=[output_str], references=[summary_string])\n",
    "\n",
    "            evals.append((summary[kThreadId], output_str, summary_string ,rouge_score))\n",
    "    \n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = evaluate(model, test[:4], nn.CrossEntropyLoss(), rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in evals:\n",
    "    thread_id = i[0]\n",
    "    output = i[1]\n",
    "    summary = i[2]\n",
    "    score = i[3]\n",
    "    print(f\"Thread ID: {thread_id}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print(f\"Summary: {summary}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summarize(\"hello world\", mode=\"top_p\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
