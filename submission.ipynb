{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections.abc\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm as progressbar\n",
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETAILS_JSON = \"data/email_thread_details.json\"\n",
    "SUMMARIES_JSON = \"data/email_thread_summaries.json\"\n",
    "\n",
    "kThreadId = \"thread_id\"\n",
    "kSubject = \"subject\"\n",
    "kTimestamp = \"timestamp\"\n",
    "kFrom = \"from\"\n",
    "kTo = \"to\"\n",
    "kBody = \"body\"\n",
    "\n",
    "kSummary = \"summary\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "- Reduced vocab from 172743 to 16856\n",
    "- `#` WOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils():\n",
    "    @staticmethod\n",
    "    def load_dataset(DETAILS_FILE, SUMMARIES_FILE):\n",
    "        '''\n",
    "            This function loads the dataset from the file\n",
    "            ARGS:\n",
    "                filename: the name of the file\n",
    "            RETURN:\n",
    "                dataset: the dataset\n",
    "        '''\n",
    "        with open(DETAILS_FILE, 'r') as f:\n",
    "            details = json.load(f)\n",
    "        \n",
    "        with open(SUMMARIES_FILE, 'r') as f:\n",
    "            summaries = json.load(f)   \n",
    "        \n",
    "        dataset = {}\n",
    "        for i in range(len(details)):\n",
    "            item = details[i]\n",
    "            thread_id = item[kThreadId]\n",
    "            item = Utils.tokenize_body(item)\n",
    "            dataset[thread_id] = dataset.get(thread_id, []) + [item]\n",
    "        \n",
    "        for i in range(len(summaries)):\n",
    "            item = summaries[i]\n",
    "            thread_id = item[kThreadId]\n",
    "            item = Utils.tokenize_summary(item)\n",
    "            dataset[thread_id] = (dataset.get(thread_id), item)\n",
    "\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_body(item):\n",
    "        sentences = sent_tokenize(item[kBody])\n",
    "        item[kBody] = [word_tokenize(sentence) for sentence in sentences]\n",
    "        item[kBody] = \" \".join([word for sentence in item[kBody] for word in sentence])\n",
    "        item[kBody] = \"<BOS> \" + item[kBody] + \" <EOS>\"\n",
    "        return item\n",
    "        item[kBody] = [re.sub(r'[^\\w\\s.]', '', word) for word in item[kBody]]\n",
    "        item[kBody] = [word.strip() for word in item[kBody] if word.strip() and word.strip() not in ['--', '=']]\n",
    "        # Lowercase the email body\n",
    "        item[kBody] = [word.lower() for word in item[kBody]]\n",
    "        item[kBody] = [\"<BOS>\"] + item[kBody] + [\"<EOS>\"]\n",
    "        item[kBody] = \" \".join(item[kBody])\n",
    "        return item\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_summary(item):\n",
    "        sentences = sent_tokenize(item[kSummary])\n",
    "        item[kSummary] = [word_tokenize(sentence) for sentence in sentences]\n",
    "        item[kSummary] = \" \".join([word for sentence in item[kSummary] for word in sentence])\n",
    "        item[kSummary] = \"<BOS> \" + item[kSummary] + \" <EOS>\"\n",
    "        return item\n",
    "        item[kSummary] = [re.sub(r'[^\\w\\s.]', '', word) for word in item[kSummary]]\n",
    "        item[kSummary] = [word.strip() for word in item[kSummary] if word.strip() and word.strip() not in ['--', '=']]\n",
    "        # Lowercase the summary\n",
    "        item[kSummary] = [word.lower() for word in item[kSummary]]\n",
    "        item[kSummary] = \"<BOS> \" + \" \".join(item[kSummary]) + \" <EOS>\"\n",
    "        return item\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_vocab(data):\n",
    "        '''\n",
    "            This function builds the vocabulary from the data\n",
    "            ARGS:\n",
    "                data: the data to build the vocabulary from ([Email], EmailSummaries)\n",
    "            RETURN:\n",
    "                vocab: the vocabulary\n",
    "        '''\n",
    "        vocab = Vocab()\n",
    "        for _, (email_list, summary) in data.items():\n",
    "            for email in email_list:\n",
    "\n",
    "                for word in email:\n",
    "                    vocab.add(word)\n",
    "            for word in summary[kSummary].split():\n",
    "                vocab.add(word)\n",
    "        \n",
    "        return vocab\n",
    "\n",
    "class Vocab(collections.abc.MutableSet):\n",
    "    \"\"\"\n",
    "        Set-like data structure that can change words into numbers and back.\n",
    "        From Prof. David Chiang Code\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        words = {'<BOS>', '<EOS>', '<UNK>'}\n",
    "        self.num_to_word = list(words)\n",
    "        self.word_to_num = {word:num for num, word in enumerate(self.num_to_word)}\n",
    "    def add(self, word):\n",
    "        if word in self: return\n",
    "        num = len(self.num_to_word)\n",
    "        self.num_to_word.append(word)\n",
    "        self.word_to_num[word] = num\n",
    "    def discard(self, word):\n",
    "        raise NotImplementedError()\n",
    "    def update(self, words):\n",
    "        self |= words\n",
    "    def __contains__(self, word):\n",
    "        return word in self.word_to_num\n",
    "    def __len__(self):\n",
    "        return len(self.num_to_word)\n",
    "    def __iter__(self):\n",
    "        return iter(self.num_to_word)\n",
    "\n",
    "    def numberize(self, word):\n",
    "        \"\"\"Convert a word into a number.\"\"\"\n",
    "        if word in self.word_to_num:\n",
    "            return self.word_to_num[word]\n",
    "        else:\n",
    "            return self.word_to_num['<UNK>']\n",
    "\n",
    "    def denumberize(self, num):\n",
    "        \"\"\"Convert a number into a word.\"\"\"\n",
    "        return self.num_to_word[num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "d = Utils.load_dataset(DETAILS_JSON, SUMMARIES_JSON)\n",
    "vocab = Utils.build_vocab(d)\n",
    "len_vocab = len(vocab)\n",
    "print(\"Vocab Size: \", len_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dictionary into train and test\n",
    "data = list(d.items())\n",
    "random.shuffle(data)\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train = [(email_list, summary) for _, (email_list, summary) in train]\n",
    "test = [(email_list, summary) for _, (email_list, summary) in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer(nn.Transformer):\n",
    "    '''\n",
    "    This class implements the summarizer\n",
    "    '''\n",
    "\n",
    "    def __init__(self, vocab_size, vocab, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        '''\n",
    "        This function initializes the model\n",
    "        ARGS:\n",
    "            vocab_size: the size of the vocabulary\n",
    "            d_model: the dimension of the model\n",
    "            nhead: the number of heads\n",
    "            num_encoder_layers: the number of encoder layers\n",
    "            num_decoder_layers: the number of decoder layers\n",
    "        RETURN:\n",
    "            None\n",
    "        '''\n",
    "\n",
    "        super(Summarizer, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # Embedding layer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_encoder_layers, \n",
    "            num_decoder_layers=num_decoder_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.kGreedy = \"greedy\"\n",
    "        self.kTopP = \"top_p\"\n",
    "        self.kBeam = \"beam\"\n",
    "\n",
    "    def forward(self, src, target=None):\n",
    "        '''\n",
    "        This function performs the forward pass of the model\n",
    "        ARGS:\n",
    "            src: the source input\n",
    "            target: the target input (optional, used during training)\n",
    "        RETURN:\n",
    "            output: the output of the model\n",
    "        '''\n",
    "        # reduce the size of the src\n",
    "        kenel_size = 10\n",
    "        stride = 5\n",
    "        # src = src.unsqueeze(0)\n",
    "        # src = src.squeeze(0)\n",
    "        src = self.embedding(src)\n",
    "        src = torch.transpose(src, 0, 1)\n",
    "        src = src.unsqueeze(1)\n",
    "        src = torch.nn.functional.avg_pool1d(src, kenel_size, stride)\n",
    "        src = src.squeeze(1)\n",
    "        src = torch.transpose(src, 0, 1)\n",
    "        \n",
    "\n",
    "        \n",
    "        if target is not None:\n",
    "            target = target[:-1]\n",
    "            target = self.embedding(target)\n",
    "            output = self.transformer(src, target)\n",
    "        else:\n",
    "            # In generation mode, don't use target\n",
    "            output = self.transformer(src, src)  # Use src as both source and target TODO: \n",
    "        \n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def summarize(self, src, max_len=100, mode=\"top_p\"):\n",
    "        '''\n",
    "        This function summarize the input text\n",
    "            args:\n",
    "                src: the source input\n",
    "                max_len: the maximum length of the output\n",
    "                mode: the mode of generation (greedy or beam search)\n",
    "            return:\n",
    "                output: the output of the model\n",
    "        '''\n",
    "        src = torch.tensor([self.vocab.numberize(word) for word in src])\n",
    "        o = self.forward(src)\n",
    "        output = None\n",
    "        \n",
    "        if mode == self.kGreedy:\n",
    "            output =  self.greedy_decoding(o, max_len)\n",
    "        elif mode == self.kTopP:\n",
    "            output = self.top_p_decoding(o, max_len)\n",
    "        elif mode == self.kBeam:\n",
    "            output = self.beam_search(o, max_len)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def greedy_decoding(self, o, max_len):\n",
    "        '''\n",
    "        This function performs greedy decoding\n",
    "        ARGS:\n",
    "            o: the output of the model\n",
    "            max_len: the maximum length of the output\n",
    "        RETURN:\n",
    "            output: the output of the model\n",
    "        '''\n",
    "        output = []\n",
    "        words = 0\n",
    "        for i in o:\n",
    "            if words >= max_len:\n",
    "                break\n",
    "            a = torch.argmax(i)\n",
    "            if a == self.vocab.numberize(\"<EOS>\"):\n",
    "                break\n",
    "            a = self.vocab.denumberize(a)\n",
    "            output.append(a)\n",
    "            words += 1\n",
    "        return output\n",
    "    \n",
    "    def top_p_decoding(self, o, max_len = 50, p=0.9):\n",
    "        '''\n",
    "        This function performs top-p decoding\n",
    "        ARGS:\n",
    "            o: the output of the model\n",
    "            max_len: the maximum length of the output\n",
    "            p: the probability threshold\n",
    "        RETURN:\n",
    "            output: the output of the model\n",
    "        '''\n",
    "        output = []\n",
    "        words = 0\n",
    "        for i in o:\n",
    "            if words >= max_len:\n",
    "                break\n",
    "            sorted_prob, sorted_idx = torch.sort(i, descending=False)\n",
    "            sorted_prob = torch.exp(sorted_prob)\n",
    "            sorted_prob_cumsum = sorted_prob.cumsum(dim=0)\n",
    "            top_p_batch = sorted_idx[sorted_prob_cumsum > p]\n",
    "\n",
    "            if top_p_batch.nelement() > 0:\n",
    "                next_token = random.choice(top_p_batch)\n",
    "                output.append(next_token.item())\n",
    "            else:\n",
    "                next_token = random.choice(sorted_idx)\n",
    "                output.append(next_token.item())\n",
    "            \n",
    "            if output[-1] == self.vocab.numberize(\"<EOS>\"):\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        for i, tensor in enumerate(output):\n",
    "            output[i] = self.vocab.denumberize(tensor)\n",
    "    \n",
    "        return output\n",
    "\n",
    "    def beam_search(self, o, max_len):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab) # size of vocabulary\n",
    "emsize = 160 # embedding dimension\n",
    "nhid = 160 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "n_encoder_layers = 6 # the number of encoder layers\n",
    "n_decoder_layers = 6 # the number of decoder layers\n",
    "nhead = 8 # the number of heads in the multiheadattention models\n",
    "lr = 0.001 # learning rate\n",
    "model = Summarizer(ntokens, vocab, emsize, nhead, n_encoder_layers, n_decoder_layers).to(DEVICE)\n",
    "\n",
    "# RECENT_MODEL = \"models/model.pt-2023-12-04_15_40_57.pt\"\n",
    "# model.load_state_dict(torch.load(RECENT_MODEL, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_summarizer(model: Summarizer, train_data, dev_data, criterion, epochs=1, lr=0.003):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Initialize Adam optimizer\n",
    "    prev_dev_loss = best_dev_loss = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1} / {epochs}\")\n",
    "        random.shuffle(train_data)\n",
    "        model.train()  # Turn on the train mode\n",
    "        train_loss = 0\n",
    "\n",
    "        for item in progressbar.tqdm(train_data, desc=\"Thread Training\", total=len(train_data)):\n",
    "            thread, summary = item\n",
    "            email_thread_body = [email[kBody] for email in thread]\n",
    "            email_thread_body = \" \".join(email_thread_body)\n",
    "\n",
    "            email_tensor = torch.tensor([model.vocab.numberize(word) for word in email_thread_body.split()], dtype=torch.int64).to(DEVICE) # Convert email to tensor\n",
    "            summary_tensor = torch.tensor([model.vocab.numberize(word) for word in summary[kSummary].split()],dtype=torch.int64).to(DEVICE) # Convert summary to tensor\n",
    "            \n",
    "            output = model(email_tensor, summary_tensor)  # Forward pass\n",
    "            summary_tensor = summary_tensor[1:] # Remove <BOS> token\n",
    "            loss = criterion(output, summary_tensor) # Calculate loss\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            loss.backward() # Backward pass\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), threshold_norm) # Clip gradients\n",
    "            optimizer.step() # Update weights\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        dev_loss = 0\n",
    "        evals = []\n",
    "        line_num = 0\n",
    "        for item in progressbar.tqdm(dev_data, desc=\"Thread Dev\", total=len(dev_data)):\n",
    "            thread, summary = item\n",
    "            summary_string = summary[kSummary]\n",
    "\n",
    "            email_thread_body = [email[kBody] for email in thread]\n",
    "            email_thread_body = \" \".join(email_thread_body)\n",
    "\n",
    "            output = model.summarize(email_thread_body, mode=\"top_p\", max_len=40)\n",
    "            output = \" \".join(output)\n",
    "            score = rouge.compute(predictions=[output], references=[summary_string])\n",
    "            evals.append((summary[kThreadId], output, summary_string, score))\n",
    "            if line_num < 5 and epoch % 5 == 0:\n",
    "                print(f\"Thread ID: {summary[kThreadId]}\")\n",
    "                print(f\"Email Thread: {email_thread_body}\")\n",
    "                print(f\"Predicted Summary: {output}\")\n",
    "                print(f\"Actual Summary: {summary_string}\")\n",
    "                print(f\"Score: {score}\")\n",
    "                print(\"-----------------------------------\")\n",
    "            line_num += 1\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}: Average Train Loss: {train_loss/len(train_data)}\",file=sys.stderr ,flush=True)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train[:10]\n",
    "dev_subset = test[:10]\n",
    "\n",
    "curr_time = time.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "MODEL_PATH = f\"models/model.pt-{curr_time}.pt\"\n",
    "\n",
    "model = train_summarizer(\n",
    "    model=model, \n",
    "    train_data=train, \n",
    "    dev_data=train_subset, \n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    epochs=10,\n",
    ")\n",
    "# save the model\n",
    "\n",
    "# torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join([email[kBody] for email in train_subset[0][0]])\n",
    "summary = train_subset[0][1][kSummary]\n",
    "o = model.summarize(text, max_len=50, mode=\"top_p\")\n",
    "o = \" \".join(o)\n",
    "print(o)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "# RECENT_MODEL = \"models/model.pt-2023-12-05_18:16:19.pt\"\n",
    "# model.load_state_dict(torch.load(RECENT_MODEL, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the output\n",
    "\n",
    "def evaluate(model: Summarizer, test_data, criterion, rouge, max_input_len = 150, max_output_len = 50, mode=\"greedy\"):\n",
    "    model.eval()  # Turn on the evaluation mode\n",
    "    \n",
    "    total_loss = 0.\n",
    "    evals = []\n",
    "    with torch.no_grad():\n",
    "        for item in progressbar.tqdm(test_data, desc=\"Thread Evaluation\", total=len(test_data)):\n",
    "            thread, summary = item\n",
    "            summary_string = summary[kSummary]\n",
    "\n",
    "            email_thread_body = [email[kBody] for email in thread]\n",
    "            email_thread_body = \" \".join(email_thread_body)\n",
    "\n",
    "            output = model.summarize(email_thread_body, mode=\"top_p\")\n",
    "        \n",
    "            # loss = criterion(output, summary_string)\n",
    "            # total_loss += loss.item()\n",
    "            output_str = \" \".join(output)\n",
    "            rouge_score = rouge.compute(predictions=[output_str], references=[summary_string])\n",
    "\n",
    "            evals.append((summary[kThreadId], output_str, summary_string ,rouge_score))\n",
    "    \n",
    "    return evals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
