{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7133688,"sourceType":"datasetVersion","datasetId":4116047}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import collections.abc\nimport json\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nimport math\nimport tqdm as progressbar\nimport time\nimport enum\nimport nltk\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\nimport re","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DETAILS_JSON = \"/kaggle/input/subject-generation/email_thread_details.json\"\n# SUMMARIES_JSON = \"data/email_thread_summaries.json\"\n\nkThreadId = \"thread_id\"\nkSubject = \"subject\"\nkTimestamp = \"timestamp\"\nkFrom = \"from\"\nkTo = \"to\"\nkBody = \"body\"\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"\n\nclass Utils():\n    @staticmethod\n    def load_dataset(DETAILS_FILE):\n        '''\n            This function loads the dataset from the file\n            ARGS:\n                filename: the name of the file\n            RETURN:\n                dataset: the dataset\n        '''\n        with open(DETAILS_FILE, 'r') as f:\n            details = json.load(f)\n        \n        \n        dataset = {}\n        for i in range(len(details)):\n            item = details[i]\n            thread_id = item[kThreadId]\n            body= Utils.tokenize(item)\n            dataset[thread_id] = dataset.get(thread_id, []) + [body[kBody]]\n            \n        \n        \n        curr_thread = 0\n        for i in range(len(details)):\n            item = details[i]\n            thread_id = item[kThreadId]\n            if thread_id != curr_thread:\n                curr_thread = thread_id\n                subject = Utils.tokenize_subject(item)\n                dataset[thread_id] = (dataset.get(thread_id), subject)\n\n\n        return dataset\n    \n    @staticmethod\n    def tokenize(item):\n        item[kBody] = word_tokenize(item[kBody])\n        item[kBody] = [re.sub(r'[^\\w\\s.]', '', word) for word in item[kBody]]\n        item[kBody] = [word.strip() for word in item[kBody] if word.strip() and word.strip() not in ['--', '=']]\n        # Lowercase the email body\n        item[kBody] = [word.lower() for word in item[kBody]]\n        item[kBody] = [\"<BOS>\"] + item[kBody] + [\"<EOS>\"]\n        item[kBody] = \" \".join(item[kBody])\n        return item\n        \n    def tokenize_subject(item):\n        # repeat with the subject which is the same for each thread id\n        item[kSubject] = word_tokenize(item[kSubject])\n        item[kSubject] = [re.sub(r'[^\\w\\s.]', '', word) for word in item[kSubject]]\n        item[kSubject] = [word.strip() for word in item[kSubject] if word.strip() and word.strip() not in ['--', '=']]\n        # Lowercase the email subject\n        item[kSubject] = [word.lower() for word in item[kSubject]]\n        item[kSubject] = [\"<BOS>\"] + item[kSubject] + [\"<EOS>\"]\n        item[kSubject] = \" \".join(item[kSubject])\n        return item[kSubject]\n    \n    @staticmethod\n    def build_vocab(data):\n        '''\n            This function builds the vocabulary from the data\n            ARGS:\n                data: the data to build the vocabulary from ([Email], EmailSummaries)\n            RETURN:\n                vocab: the vocabulary\n        '''\n        vocab = Vocab()\n        for _, (email_list, subject) in data.items():\n            for email in email_list:\n#                 print(email_list)\n                for word in email:\n#                     print(word)\n                    vocab.add(word)\n                for word in subject.split():\n                    vocab.add(word)\n            \n        \n        return vocab\n\nclass Vocab(collections.abc.MutableSet):\n    \"\"\"\n        Set-like data structure that can change words into numbers and back.\n        From Prof. David Chiang Code\n    \"\"\"\n    def __init__(self):\n        words = {'<BOS>', '<EOS>', '<UNK>'}\n        self.num_to_word = list(words)\n        self.word_to_num = {word:num for num, word in enumerate(self.num_to_word)}\n    def add(self, word):\n        if word in self: return\n        num = len(self.num_to_word)\n        self.num_to_word.append(word)\n        self.word_to_num[word] = num\n    def discard(self, word):\n        raise NotImplementedError()\n    def update(self, words):\n        self |= words\n    def __contains__(self, word):\n        return word in self.word_to_num\n    def __len__(self):\n        return len(self.num_to_word)\n    def __iter__(self):\n        return iter(self.num_to_word)\n\n    def numberize(self, word):\n        \"\"\"Convert a word into a number.\"\"\"\n        if word in self.word_to_num:\n            return self.word_to_num[word]\n        else:\n            return self.word_to_num['<UNK>']\n\n    def denumberize(self, num):\n        \"\"\"Convert a number into a word.\"\"\"\n        return self.num_to_word[num]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre Process","metadata":{}},{"cell_type":"code","source":"# Load the data\nd = Utils.load_dataset(DETAILS_JSON)\nfor i, (key, value) in enumerate(d.items()):\n    if key == 4:\n        print(key,value)\n    if key > 4:\n        break\nvocab = Utils.build_vocab(d)\n\n# 4 (['<BOS> thanks for the update . pl <EOS>', '<BOS> that is ok . thanks for the update . pl <EOS>', '<BOS> i just went to look at the file and the data is yesterday s data . we need the current prices that are set each day at roughly 1210 for the gas day of the next day . please let me know if this does not make sense . thanks . pl <EOS>', '<BOS> today s file looks good . thanks for your help . pl <EOS>'], '<BOS> eol data <EOS>')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the dictionary into train and test\ndata = list(d.items())\n# print(data[3])\nrandom.shuffle(data)\ntrain, test = train_test_split(data, test_size=0.2, random_state=42)\n\ntrain = [email_list for _, email_list in train]\n# print(train[1])\ntest = [email_list for _, email_list in test]","metadata":{"execution":{"iopub.status.busy":"2023-12-06T17:03:35.349812Z","iopub.execute_input":"2023-12-06T17:03:35.350684Z","iopub.status.idle":"2023-12-06T17:03:35.365592Z","shell.execute_reply.started":"2023-12-06T17:03:35.350650Z","shell.execute_reply":"2023-12-06T17:03:35.364576Z"},"trusted":true},"execution_count":115,"outputs":[{"name":"stdout","text":"(4, (['<BOS> thanks for the update . pl <EOS>', '<BOS> that is ok . thanks for the update . pl <EOS>', '<BOS> i just went to look at the file and the data is yesterday s data . we need the current prices that are set each day at roughly 1210 for the gas day of the next day . please let me know if this does not make sense . thanks . pl <EOS>', '<BOS> today s file looks good . thanks for your help . pl <EOS>'], '<BOS> eol data <EOS>'))\n(['<BOS> paul you say 5000day plus fuel . the delivery point is in the south texas pool . where does the fuel come in and how is it determined also did we ever determine the name of the reliant energy entity that was doing the deal paul burgener enron 09252000 0445 pm to dan j hyvlhouect ect cc subject trunkline deals wreliant dan attached are the details for the enovate transaction s purchases of physical gas from reliant trunkline stx pool . i received your draft of the agreed master for the 30000 day parcel . could you draft a form for the 5000day parcel winter only . i need this turned around so i can forward to the reliant people asap . i also need to forward any changes to the agreement past our partners pec legals folks . thanks if you have any questions please call me at 312 5411226 <EOS>', '<BOS> dan the company is reliant energy wholesale trading group at this point and the fuel is handled the same as the 30000d deal that you wrote up . see attached thanks <EOS>', '<BOS> paul please check with your counterparty at reliant for the full name of the reliant energy entity . is it reliant energy gas resources corp. or some other name i am still confused about the fuel component of the volume . we probably need to expand on the language in the prior draft to specify the fuel volume for the movement of the gas from the delivery point to ___________________ . have the attached documents been sent to reliant energy and if so have we received any comments from them i will draft a transaction agreement for the latest deal that will be similar to the prior deal . paul burgener enron 09262000 1006 am to dan j hyvlhouect ect cc subject re trunkline deals wreliant dan the company is reliant energy wholesale trading group at this point and the fuel is handled the same as the 30000d deal that you wrote up . see attached thanks <EOS>', '<BOS> attached is a draft transaction agreement for the 5000day winter deal . please review . <EOS>', '<BOS> dan the legal name is reliant energy services inc . i have forwarded the two transactions to reliant and as of yet i have not gotten any comments back . i will forward those to you as soon as i receive them . thanks for all help . <EOS>'], '<BOS> trunkline deals wreliant <EOS>')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"class SubjectGenerator(nn.Transformer):\n    '''\n    This class implements the SubjectGenerator\n\n    \n    '''\n\n    def __init__(self, vocab_size, vocab, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6):\n        '''\n        This function initializes the model\n        ARGS:\n            vocab_size: the size of the vocabulary\n            d_model: the dimension of the model\n            nhead: the number of heads\n            num_encoder_layers: the number of encoder layers\n            num_decoder_layers: the number of decoder layers\n        RETURN:\n            None\n        '''\n\n        super(SubjectGenerator, self).__init__()\n        self.model_type = 'Transformer'\n        self.d_model = d_model\n        self.embedding = nn.Embedding(vocab_size, d_model) # Embedding layer\n        self.transformer = nn.Transformer(\n            d_model=d_model, \n            nhead=nhead, \n            num_encoder_layers=num_encoder_layers, \n            num_decoder_layers=num_decoder_layers\n        )\n        self.fc_out = nn.Linear(d_model, vocab_size)\n        self.vocab = vocab\n\n    def forward(self, src, target=None):\n        '''\n        This function performs the forward pass of the model\n        ARGS:\n            src: the source input\n            target: the target input (optional, used during training)\n        RETURN:\n            output: the output of the model\n        '''\n        src = self.embedding(src)\n        \n        if target is not None:\n            target = self.embedding(target)\n            output = self.transformer(src, target)\n        else:\n            # In generation mode, don't use target\n            output = self.transformer(src, src)  # Use src as both source and target TODO: \n        \n        output = self.fc_out(output)\n\n        return output\n    \n    def summarize(self, src, max_len=100, mode=\"greedy\"):\n        '''\n        This function summarize the input text\n            args:\n                src: the source input\n                max_len: the maximum length of the output\n                mode: the mode of generation (greedy or beam search)\n            return:\n                output: the output of the model\n        '''\n        o = self.forward(src)\n        output = None\n        \n        if mode == \"greedy\":\n            output =  self.greedy_decoding(o, max_len)\n        elif mode == \"beam\":\n            output = self.beam_search(o, max_len)\n\n        return output\n        \n    def greedy_decoding(self, o, max_len):\n        '''\n        This function performs greedy decoding\n        ARGS:\n            o: the output of the model\n            max_len: the maximum length of the output\n        RETURN:\n            output: the output of the model\n        '''\n        output = []\n        words = 0\n        for i in o:\n            if words >= max_len:\n                break\n            a = torch.argmax(i)\n            a = self.vocab.denumberize(a)\n            output.append(a)\n            words += 1\n        return output\n\n    def beam_search(self, o, max_len):\n        '''\n        \n        '''\n        pass","metadata":{"execution":{"iopub.status.busy":"2023-12-06T17:51:25.860432Z","iopub.execute_input":"2023-12-06T17:51:25.860832Z","iopub.status.idle":"2023-12-06T17:51:25.874024Z","shell.execute_reply.started":"2023-12-06T17:51:25.860801Z","shell.execute_reply":"2023-12-06T17:51:25.873054Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"ntokens = len(vocab) # size of vocabulary\nemsize = 100 # embedding dimension\nnhid = 100 # the dimension of the feedforward network model in nn.TransformerEncoder\nn_encoder_layers = 6 # the number of encoder layers\nn_decoder_layers = 6 # the number of decoder layers\nnhead = 2 # the number of heads in the multiheadattention models\nlr = 0.02 # learning rate\n\nmodel = SubjectGenerator(ntokens, vocab, emsize, nhead, n_encoder_layers, n_decoder_layers)\n\n# RECENT_MODEL = \"models/model.pt-2023-12-04_00:29:43.pt\"\n# model.load_state_dict(torch.load(RECENT_MODEL))","metadata":{"execution":{"iopub.status.busy":"2023-12-06T17:51:36.497223Z","iopub.execute_input":"2023-12-06T17:51:36.498134Z","iopub.status.idle":"2023-12-06T17:51:36.982241Z","shell.execute_reply.started":"2023-12-06T17:51:36.498095Z","shell.execute_reply":"2023-12-06T17:51:36.981451Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"def train_SubjectGenerator(model: SubjectGenerator, train_data, criterion, max_input_len = 150, max_output_len = 50, lr=0.001, threshold_norm=0.5):\n    model.train()  # Turn on the train mode\n    total_loss = 0.\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Initialize Adam optimizer\n\n    for item in progressbar.tqdm(train_data, desc=\"Thread Training\", total=len(train_data)):\n        optimizer.zero_grad()  # Zero the gradients\n        thread = item\n#         print(thread)\n        bodies, subject = thread\n#         print(bodies, subject)\n\n        body = \" \".join(bodies)\n        email_tensor = torch.tensor([model.vocab.numberize(word) for word in body.split()]) # Convert email to tensor\n        subject_tensor = torch.tensor([model.vocab.numberize(word) for word in subject.split()]) # Convert subject to tensor\n\n\n        if email_tensor.nelement() > max_input_len: # Truncate email if it is too long\n            email_tensor = email_tensor[:max_input_len]\n\n        output = model(email_tensor, subject_tensor)  # Forward pass\n\n        output = output.view(-1, ntokens) # Reshape output \n        loss = criterion(output, subject_tensor) # Calculate loss\n        loss.backward() # Backward pass\n        torch.nn.utils.clip_grad_norm_(model.parameters(), threshold_norm) # Clip gradients\n        optimizer.step() # Update weights\n        total_loss += loss.item() \n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T17:51:40.695392Z","iopub.execute_input":"2023-12-06T17:51:40.696466Z","iopub.status.idle":"2023-12-06T17:51:40.707960Z","shell.execute_reply.started":"2023-12-06T17:51:40.696415Z","shell.execute_reply":"2023-12-06T17:51:40.706810Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"curr_time = time.strftime(\"%Y-%m-%d_%H:%M:%S\")\nMODEL_PATH = f\"models/model.pt-{curr_time}.pt\"\n\nmodel = train_SubjectGenerator(model, train, nn.CrossEntropyLoss(), lr=lr)\n# model.train()  # Turn on the train mode\n#     total_loss = 0.\n\n#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Initialize Adam optimizer\n\n#     for item in progressbar.tqdm(train_data, desc=\"Thread Training\", total=len(train_data)):\n#         optimizer.zero_grad()  # Zero the gradients\n#         thread = item\n# #         print(thread)\n#         bodies, subject = thread\n# #         print(bodies, subject)\n\n#         body = \" \".join(bodies)\n#         email_tensor = torch.tensor([model.vocab.numberize(word) for word in body.split()]) # Convert email to tensor\n#         subject_tensor = torch.tensor([model.vocab.numberize(word) for word in subject.split()]) # Convert subject to tensor\n\n\n#         if email_tensor.nelement() > max_input_len: # Truncate email if it is too long\n#             email_tensor = email_tensor[:max_input_len]\n\n#         output = model(email_tensor, subject_tensor)  # Forward pass\n\n#         output = output.view(-1, ntokens) # Reshape output \n#         loss = criterion(output, subject_tensor) # Calculate loss\n#         loss.backward() # Backward pass\n#         torch.nn.utils.clip_grad_norm_(model.parameters(), threshold_norm) # Clip gradients\n#         optimizer.step() # Update weights\n#         total_loss += loss.item()\n# save the model\ntorch.save(model.state_dict(), MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T17:51:44.468308Z","iopub.execute_input":"2023-12-06T17:51:44.469049Z","iopub.status.idle":"2023-12-06T17:59:40.421684Z","shell.execute_reply.started":"2023-12-06T17:51:44.469011Z","shell.execute_reply":"2023-12-06T17:59:40.420308Z"},"trusted":true},"execution_count":134,"outputs":[{"name":"stderr","text":"Thread Training: 100%|██████████| 3333/3333 [07:55<00:00,  7.01it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[134], line 34\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m train_SubjectGenerator(model, train, nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# model.train()  # Turn on the train mode\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     total_loss = 0.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#         total_loss += loss.item()\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# save the model\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mRuntimeError\u001b[0m: Parent directory models does not exist."],"ename":"RuntimeError","evalue":"Parent directory models does not exist.","output_type":"error"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:04:32.910796Z","iopub.execute_input":"2023-12-06T18:04:32.911205Z","iopub.status.idle":"2023-12-06T18:04:33.164296Z","shell.execute_reply.started":"2023-12-06T18:04:32.911164Z","shell.execute_reply":"2023-12-06T18:04:33.163385Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"!pip install evaluate\n!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:05:16.466360Z","iopub.execute_input":"2023-12-06T18:05:16.467046Z","iopub.status.idle":"2023-12-06T18:05:45.033870Z","shell.execute_reply.started":"2023-12-06T18:05:16.467008Z","shell.execute_reply":"2023-12-06T18:05:45.032655Z"},"trusted":true},"execution_count":138,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.17.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.24.3)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=025d51010b7d1abdbadabc4c3b62e465da80390f62a2055e701af510b4cde459\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nrouge = evaluate.load('rouge')","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:23:44.879198Z","iopub.execute_input":"2023-12-06T18:23:44.880757Z","iopub.status.idle":"2023-12-06T18:23:45.299723Z","shell.execute_reply.started":"2023-12-06T18:23:44.880715Z","shell.execute_reply":"2023-12-06T18:23:45.298946Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"# load the model\nRECENT_MODEL = \"/kaggle/working/models/model.pt-2023-12-06_17:51:44.pt\"\nmodel.load_state_dict(torch.load(RECENT_MODEL))","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:24:23.410662Z","iopub.execute_input":"2023-12-06T18:24:23.411527Z","iopub.status.idle":"2023-12-06T18:24:23.604411Z","shell.execute_reply.started":"2023-12-06T18:24:23.411488Z","shell.execute_reply":"2023-12-06T18:24:23.602925Z"},"trusted":true},"execution_count":142,"outputs":[{"execution_count":142,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the output\n\ndef evaluate(model: SubjectGenerator, test_data, criterion, rouge, max_input_len = 150, max_output_len = 50, mode=\"greedy\"):\n    model.eval()  # Turn on the evaluation mode\n    \n    total_loss = 0.\n    evals = []\n    with torch.no_grad():\n        for i, item in enumerate(progressbar.tqdm(test_data, desc=\"Thread Evaluation\", total=len(test_data))):\n            thread = item\n            bodies, subject = thread\n#             email_tensors = []\n            subject_string = subject\n            \n            # create tensors for bodies\n            body = \" \".join(bodies)\n            email_tensor = torch.tensor([model.vocab.numberize(word) for word in body.split()]) # Convert email to tensor\n            # trim email if it is too long\n            trim_len = math.ceil(max_input_len / len(thread))\n            if email_tensor.nelement() > trim_len: # Truncate email if it is too long\n                email_tensor = email_tensor[:trim_len]\n                \n            # Concatenate email tensors\n#             email_tensors.append(email_tensor)\n\n#             email_tensor_final = torch.cat(email_tensors, dim=0)\n\n            output = model.summarize(email_tensor)\n            # loss = criterion(output, summary_string)\n            # total_loss += loss.item()\n            output_str = \" \".join(output)\n            rouge_score = rouge.compute(predictions=[output_str], references=[subject_string])\n\n            evals.append((i+1, output_str, subject_string ,rouge_score))\n    \n    return evals","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:31:52.592686Z","iopub.execute_input":"2023-12-06T18:31:52.593128Z","iopub.status.idle":"2023-12-06T18:31:52.603164Z","shell.execute_reply.started":"2023-12-06T18:31:52.593091Z","shell.execute_reply":"2023-12-06T18:31:52.602287Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"evals = evaluate(model, test, nn.CrossEntropyLoss(), rouge)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:32:01.288787Z","iopub.execute_input":"2023-12-06T18:32:01.289159Z","iopub.status.idle":"2023-12-06T18:35:34.439070Z","shell.execute_reply.started":"2023-12-06T18:32:01.289126Z","shell.execute_reply":"2023-12-06T18:35:34.438121Z"},"trusted":true},"execution_count":149,"outputs":[{"name":"stderr","text":"Thread Evaluation: 100%|██████████| 834/834 [03:33<00:00,  3.91it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in evals:\n    thread_id = i[0]\n    output = i[1]\n    subject = i[2]\n    score = i[3]\n    print(f\"Thread ID: {thread_id}\")\n    print(f\"Output: {output}\")\n    print(f\"Subject: {subject}\")\n    print(f\"Score: {score}\")\n    if thread_id > 3:\n        break","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:35:42.837396Z","iopub.execute_input":"2023-12-06T18:35:42.837814Z","iopub.status.idle":"2023-12-06T18:35:42.844397Z","shell.execute_reply.started":"2023-12-06T18:35:42.837776Z","shell.execute_reply":"2023-12-06T18:35:42.843455Z"},"trusted":true},"execution_count":150,"outputs":[{"name":"stdout","text":"Thread ID: 1\nOutput: <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS>\nSubject: <BOS> re devon sfs <EOS>\nScore: {'rouge1': 0.025, 'rouge2': 0.0, 'rougeL': 0.025, 'rougeLsum': 0.025}\nThread ID: 2\nOutput: <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS>\nSubject: <BOS> canada work report <EOS>\nScore: {'rouge1': 0.025, 'rouge2': 0.0, 'rougeL': 0.025, 'rougeLsum': 0.025}\nThread ID: 3\nOutput: <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS>\nSubject: <BOS> transition issues <EOS>\nScore: {'rouge1': 0.025316455696202535, 'rouge2': 0.0, 'rougeL': 0.025316455696202535, 'rougeLsum': 0.025316455696202535}\nThread ID: 4\nOutput: <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS>\nSubject: <BOS> sd cap <EOS>\nScore: {'rouge1': 0.025316455696202535, 'rouge2': 0.0, 'rougeL': 0.025316455696202535, 'rougeLsum': 0.025316455696202535}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}