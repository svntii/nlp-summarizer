{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "! pip install datasets\n",
    "! pip install rouge_score\n",
    "! pip install wandb\n",
    "! pip3 install torch\n",
    "! pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import time\n",
    "import evaluate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "\n",
    "\n",
    "\n",
    "DETAILS_JSON = \"data/email_thread_details.json\"\n",
    "SUMMARIES_JSON = \"data/email_thread_summaries.json\"\n",
    "\n",
    "kThreadId = \"thread_id\"\n",
    "kSubject = \"subject\"\n",
    "kTimestamp = \"timestamp\"\n",
    "kFrom = \"from\"\n",
    "kTo = \"to\"\n",
    "kBody = \"body\"\n",
    "\n",
    "kThread = \"thread\"\n",
    "\n",
    "kSummary = \"summary\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils():\n",
    "    @staticmethod\n",
    "    def load_dataset(DETAILS_FILE, SUMMARIES_FILE):\n",
    "        '''\n",
    "            This function loads the dataset from the file\n",
    "            ARGS:\n",
    "                filename: the name of the file\n",
    "            RETURN:\n",
    "                dataset: the dataset\n",
    "        '''\n",
    "        with open(DETAILS_FILE, 'r') as f:\n",
    "            details = json.load(f)\n",
    "        \n",
    "        with open(SUMMARIES_FILE, 'r') as f:\n",
    "            summaries = json.load(f)   \n",
    "        \n",
    "        dataset = {}\n",
    "        for i in range(len(details)):\n",
    "            item = details[i]\n",
    "            thread_id = item[kThreadId]\n",
    "            item = Utils.tokenize_body(item)\n",
    "            dataset[thread_id] = dataset.get(thread_id, []) + [item]\n",
    "        \n",
    "        for i in range(len(summaries)):\n",
    "            item = summaries[i]\n",
    "            thread_id = item[kThreadId]\n",
    "            item = Utils.tokenize_summary(item)\n",
    "            dataset[thread_id] = (dataset.get(thread_id), item)\n",
    "\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_body(item):\n",
    "        item[kBody] = word_tokenize(item[kBody])\n",
    "        item[kBody] = [re.sub(r'[^\\w\\s.]', '', word) for word in item[kBody]]\n",
    "        item[kBody] = [word.strip() for word in item[kBody] if word.strip() and word.strip() not in ['--', '=']]\n",
    "        # Lowercase the email body\n",
    "        item[kBody] = [word.lower() for word in item[kBody]]\n",
    "        item[kBody] = [\"<BOS>\"] + item[kBody] + [\"<EOS>\"]\n",
    "        item[kBody] = \" \".join(item[kBody])\n",
    "        return item\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_summary(item):\n",
    "        item[kSummary] = word_tokenize(item[kSummary])\n",
    "        item[kSummary] = [re.sub(r'[^\\w\\s.]', '', word) for word in item[kSummary]]\n",
    "        item[kSummary] = [word.strip() for word in item[kSummary] if word.strip() and word.strip() not in ['--', '=']]\n",
    "        # Lowercase the summary\n",
    "        item[kSummary] = [word.lower() for word in item[kSummary]]\n",
    "        if len(item[kSummary] > 200):\n",
    "            item[kSummary] = item[kSummary][:200]\n",
    "        item[kSummary] = \"<BOS> \" + \" \".join(item[kSummary]) + \"<EOS>\"\n",
    "        return item\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_vocab(data):\n",
    "        '''\n",
    "            This function builds the vocabulary from the data\n",
    "            ARGS:\n",
    "                data: the data to build the vocabulary from ([Email], EmailSummaries)\n",
    "            RETURN:\n",
    "                vocab: the vocabulary\n",
    "        '''\n",
    "        vocab = Vocab()\n",
    "        for _, (email_list, summary) in data.items():\n",
    "            for email in email_list:\n",
    "\n",
    "                for word in email:\n",
    "                    vocab.add(word)\n",
    "            for word in summary[kSummary].split():\n",
    "                vocab.add(word)\n",
    "        \n",
    "        return vocab\n",
    "\n",
    "class Vocab(collections.abc.MutableSet):\n",
    "    \"\"\"\n",
    "        Set-like data structure that can change words into numbers and back.\n",
    "        From Prof. David Chiang Code\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        words = {'<BOS>', '<EOS>', '<UNK>'}\n",
    "        self.num_to_word = list(words)\n",
    "        self.word_to_num = {word:num for num, word in enumerate(self.num_to_word)}\n",
    "    def add(self, word):\n",
    "        if word in self: return\n",
    "        num = len(self.num_to_word)\n",
    "        self.num_to_word.append(word)\n",
    "        self.word_to_num[word] = num\n",
    "    def discard(self, word):\n",
    "        raise NotImplementedError()\n",
    "    def update(self, words):\n",
    "        self |= words\n",
    "    def __contains__(self, word):\n",
    "        return word in self.word_to_num\n",
    "    def __len__(self):\n",
    "        return len(self.num_to_word)\n",
    "    def __iter__(self):\n",
    "        return iter(self.num_to_word)\n",
    "\n",
    "    def numberize(self, word):\n",
    "        \"\"\"Convert a word into a number.\"\"\"\n",
    "        if word in self.word_to_num:\n",
    "            return self.word_to_num[word]\n",
    "        else:\n",
    "            return self.word_to_num['<UNK>']\n",
    "\n",
    "    def denumberize(self, num):\n",
    "        \"\"\"Convert a number into a word.\"\"\"\n",
    "        return self.num_to_word[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = datasets.Dataset.from_dict({\n",
    "#     kThread: [\" \".join([email[kBody] for email in thread])for thread, _ in raw_dataset.values()],\n",
    "#     kSummary: [summary[kSummary] for _, summary in raw_dataset.values()]\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the dataset into train, validation, and test\n",
    "# test_size = int(len(d) * 0.25)\n",
    "# train_size = len(d) - test_size\n",
    "# d = datasets.Dataset.train_test_split(d, test_size=test_size, train_size=train_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = datasets.load_dataset(\"knkarthick/dialogsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "og_model = AutoModelForSequenceClassification.from_pretrained(model_name, torch_dtype=torch.float32)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = d.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns= ['id', 'topic', 'dialogue', 'summary',]\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return rouge.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = self.compute_loss(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_time = time.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "MODEL_PATH = f\"models/model.pt-{curr_time}.pt\"\n",
    "\n",
    "assert len(d[\"train\"]) > 0, \"Training dataset is empty\"\n",
    "assert len(d[\"test\"]) > 0, \"Test dataset is empty\"\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_PATH,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1,\n",
    "    per_device_train_batch_size=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=og_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n",
    "#     source = \" \".join([email[kBody] for email in batch['email_list']])\n",
    "#     target = batch['summary'][kSummary]  \n",
    "    \n",
    "#     source_tokenized = tokenizer(source, padding=\"max_length\", truncation=True, max_length=max_source_length)\n",
    "#     target_tokenized = tokenizer(target, padding=\"max_length\", truncation=True, max_length=max_target_length)\n",
    "\n",
    "\n",
    "\n",
    "#     source_tokenized = tokenizer(\n",
    "#         source, padding=\"max_length\", truncation=True, max_length=max_source_length\n",
    "#     )\n",
    "    \n",
    "#     target_tokenized = tokenizer(\n",
    "#         target, padding=\"max_length\", truncation=True, max_length=max_target_length\n",
    "#     )\n",
    "\n",
    "#     batch = {k: v for k, v in source_tokenized.items()}\n",
    "\n",
    "#     batch[\"labels\"] = target_tokenized[\"input_ids\"]    \n",
    "    \n",
    "#     # Ignore padding in the loss\n",
    "\n",
    "#     # batch[\"labels\"] = [\n",
    "#     #     [-100 if token == tokenizer.pad_token_id else token for token in l]\n",
    "#     #     for l in target_tokenized[\"input_ids\"]\n",
    "#     # ]\n",
    "#     return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Utils.load_dataset(DETAILS_JSON, SUMMARIES_JSON) # load the dataset\n",
    "\n",
    "# data = list(dataset.items())\n",
    "# random.shuffle(data)\n",
    "# train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# train = [(email_list, summary) for _, (email_list, summary) in train]\n",
    "# test = [(email_list, summary) for _, (email_list, summary) in test]\n",
    "\n",
    "# train_dataset = datasets.Dataset.from_dict({\n",
    "#     'email_list': [email_list for email_list, _ in train],\n",
    "#     'summary': [summary for _, summary in train]\n",
    "# })\n",
    "\n",
    "# test_dataset = datasets.Dataset.from_dict({\n",
    "#     'email_list': [email_list for email_list, _ in test],\n",
    "#     'summary': [summary for _, summary in test]\n",
    "# })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Borrowed from https://github.com/huggingface/transformers/blob/master/examples/seq2seq/run_summarization.py\n",
    "\n",
    "# nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# metric = datasets.load_metric(\"rouge\")\n",
    "\n",
    "\n",
    "# def postprocess_text(preds, labels):\n",
    "#     preds = [pred.strip() for pred in preds]\n",
    "#     labels = [label.strip() for label in labels]\n",
    "\n",
    "#     # rougeLSum expects newline after each sentence\n",
    "#     preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "#     labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "#     return preds, labels\n",
    "\n",
    "\n",
    "# def compute_metrics(eval_preds):\n",
    "#     preds, labels = eval_preds\n",
    "#     if isinstance(preds, tuple):\n",
    "#         preds = preds[0]\n",
    "#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "#     # Replace -100 in the labels as we can't decode them.\n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "#     # Some simple post-processing\n",
    "#     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "#     result = metric.compute(\n",
    "#         predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "#     )\n",
    "#     # Extract a few results from ROUGE\n",
    "#     result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "#     prediction_lens = [\n",
    "#         np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "#     ]\n",
    "#     result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "#     result = {k: round(v, 4) for k, v in result.items()}\n",
    "#     return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
